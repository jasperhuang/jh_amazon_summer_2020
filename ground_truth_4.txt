Hi.
My name is Emily Weber.
I'm a machine learning specialist solutions architect at Amazon Web services.
And today we're gonna learn about Amazon SageMaker.
I hope you've enjoyed some of our previous videos.
We've learned about the manage notebook instances we learned about using built in algorithms.
We learned about how to bring your own.
And now we're gonna learn about the training and the tuning jobs.
This is your deep dive.
So first, a key point.
We need to learn every single model that's running on a sagemaker.
Training job has its own cluster.
This is an ephemeral cluster of a single ec2.
Instance.
Sometimes multiple.
ec2 instances.
That means you have a dedicated, ec2 instance that is alive for the number of seconds that your model is training.
So this means that you can develop on one ec2 instance and then literally train on another ec2 instance.
And so, if you have multiple machine learning models that you want to run and if they conflict in any capacity, you can have dedicated ec2 instances for each of those in particular.
This cluster is gonna come down immediately after your model finishes training, which is going to save you a ton of money at the end of the day.
In particular, it means no more virtual environments, no more conflicting package installations and no more resource dependency.
Right?
So you're actually not gonna get shut down if I have a team of 25 data scientists.
And if all 25 of my data scientists are trying to go after the same cluster, you're just gonna run into issues, right?
It's just gonna be tough if everyone is trying to use those cores trying to use the memory.
And if they're all using different versions of Python, different versions of open source packages.
It's really challenging to manage that.
And some people want to try new things, and that's good.
That's a core part of being a machine learning scientist is knowing about advancements in our field and then using those advancements.
And so if we're tied down to our resources in particular if I try something new, but it shuts down the rest of my team because I actually took out that cluster, it's gonna be tough.
But on sagemaker, each individual person not just has their own cluster, but they have as many clusters as they want to use.
And as you're gonna let them use based on the limits, that you're actually going to set for that account so that's that's good news.
That means we can scale out our training jobs.
So in particular how these training jobs work.
So we have an estimator.
We have a sagemaker.
Estimator and that estimator is gonna take a configuration for these active resource that we want to use.
So that's the number of ec2 instances that is dedicated to your train job.
That is the type of those ec2 instances, and that is the size of that EBS volume sitting on that instance.
So I want you to really think about this for a second here.
So if I'm training on 20 gigabytes of data relatively large set 20 20 gigabytes of data, I need to make sure that my volume size has at least that much right and probably a little bit more.
Also, If the model that I'm gonna use on that training cluster is going to be fairly large, I'm also gonna want to increase the size of my instance because I am going to run into issues if I don't set those up.
Also, if you're using some of the latest ec2 instances and you can move really quickly with your compute, you can actually reduce the total time of your training job.
Definitely something to think about.
So again, that is your algorithm container.
So that container we learned about all the places that can come from that can come from a built in algorithm that can come from you when you're bringing your own either script mode or docker container, or that can come from the machine learning marketplace.
In all those cases that's gonna live in ECR.
You're gonna have that execution roll right, And that's the role that's associated with that exact notebook.
You can go in and attach extra policies if you need to, and then that's your sagemaker.
Estimator and these core components are gonna be constant across almost every Sagemaker training job you're going to see you're going to need an estimator.
You're going to need that image, your execution roll, your estimator, your session, all those good things.
Okay, There's also another topic that we need to know about which is splitting our data for machine learning.
So let's say I have a data set and it's full of everyone in my hospital who has diabetes.
And so maybe some of the people do have diabetes, but some people don't.
And what I want to do is use a machine learning model.
So take that data set and then tell me who else is likely to have diabetes.
And so that's a pretty standard supervised learning problem where I'm just gonna get my X's and Y's and plug that into a model.
This is gonna work.
However, by splitting my data, so I'm going to start with a data set. That's basically everybody all together, right?
I've got all my columns and one column is whether or not that person has diabetes.
After I've done all my feature engineering, I'm gonna split that into a train validation and test set.
Most commonly, the training data set is gonna be anywhere from in low cases.
60% to upwards, more like 80 or 85% in some cases.
So is most of your data, right?
Is gonna go into that training set.
Then you're gonna take a portion and you're gonna call that your validation data and you'll take another portion call that your test data and then we're gonna fit our model on that training data.
But then we're gonna look at the performance of that model on our validation data.
And usually we're gonna perform a technique called hyperparameter tuning on our validation data.
And then once that's done we're going to see how well the model has done on the test data.
And that's how we can see the objective method the objective performance of our model.
Okay, so for all three of those different ways to split our data set we're going to send those up to sagemaker in what are called channels.
So we can have a train validation and test channel, and both of those need to be s3 locations.
So actually, just copy our data back to s3 before we send it up to sagemaker.
So we call it staging our features, and that's so that we can get all the versioning all the collaboration and all the availability that we get with storing our data in s3 and many of the sagemaker python SDK methods are actually gonna write that data to s3 for you.
So if that's a single function, you can just send that out, great.
So then we call Model dot fit, right?
And so the way we always call Model dot fit except what's happening is that a new cluster is coming online, so that's a new ec2 instance, and a few minutes after that's up, you're going to send the logs out to Cloudwatch.
So we're actually gonna see point by point in time what the metrics on your model are, how well it's training, what the throughput is, what your CPU utilization is, and then you can monitor that performance via the console or through a notification stream.
So once we've trained our model, we need to evaluate it and a common way of evaluating your model is with what's known as a confusion matrix.
And so the confusion matrix is gonna take your label data and the predictions that are coming out of your machine learning model, and both of those are gonna be broken up into positive and negative cases.
So when your model predicts positive and you're labeled data is positive.
We call that a true positive.
When your model is predicting negative and you're labeled data is negative.
We're gonna call that a true negative.
Both of those cases were good, right?
That's where your models adding value its operating as expected.
Over here, when your model is predicting negative at, but you're labeled, data is positive.
So you're actually missing one of those cases In this case, we're gonna call that a false negative.
And then down here, if your model is predicting positive but your label data is negative, we're gonna call that one a false positive.
So both of those are not so good, right?
Those are cases where our model is not operating as expected, and we need to improve the performance.
There are a few extra techniques we can use to do them.
There's a term we want to know about.
And that term is recall.
Recall is literally the number of true positives you have over all of the positive cases that are in your data set.
So it's literally how many positive cases you're catching so In my diabetes case, that would be whether or not I can catch all of the people in my hospital who actually have diabetes over here is another term that's called precision and precision also starts with your true positive, but it's literally true positive rate over all of the predictions that are coming out of your model on the positive side.
And so in that case, in my diabetes model, I would have great precision if every time my model predicted positive, I actually found someone who had diabetes.
But if I missed everybody, I would have bad recall.
And so those are two different ways that you can weigh the performance of your model, and in particular, we're gonna want to use both of those terms with hyperparameter tuning.
And so let's let's dive in here so hyperparameter tuning so first off a parameter that's generally the features that are going into your model.
So that's the actual data that's being supplied to your model.
That can also refer to some of the extra terms.
A hyperparameter typically refers to extra math, if you will.
It's extra terms.
It's knobs that you can supply.
They're just real numbers.
The real numbers, whether that's the maximum depth of your trees or the learning rate of your deep learning models.
And then the tuning basically refers to trying as much as you can, right?
So that's that's running a large number of jobs over time we start to gain more intuition about which hyperparameters perform better under certain settings, and so eventually there's a little bit less of guess and check.
But it's helpful to be able to just do this at scale.
And so when you're performing hyperparameter tuning and amazon sagemaker, there a couple steps.
First thing you want to do is pick the hyperparameters you're looking for and the ranges on those hyperparameters.
So if I'm trying to tune, XGBoost.
I'm going to select four hyperparameters.
In this case, eta, min child weight alpha and max depth, and then I'm also gonna get the ranges on those.
So whether it's a continuous or an integer parameter, I need to know, and then the specific ranges on those guys all that is available in the sagemaker documentation.
After that, I need my objective metric.
And so my objective metric.
That's just the term that I'm gonna use to understand how well my model is performing.
We're gonna pull that objective metric from one of your channels and it's never the training set, right?
It's always either the validation or the test set.
And that's so that we know that your model's actually learning something beyond what it was expressly trained on.
And so in this case, we're going to get AUC on a validation data AUC stands for area under the curve, typically under the receiver operator.
All right, then we're gonna pick our job parameters.
And so we've got our estimator.
We know the objective Metric.
We know our ranges.
Basically, we need to tell sagemaker the total number of jobs we want to run.
In this case, we're going to run 20 jobs.
Also, the number of jobs in parallel that we want to run.
And so once we're finished, it's gonna look like this actual on the X axis, We're gonna have time.
y axis is gonna be objective.
Metric.
And so, for each point in time, Sagemaker is going to be training up to three models for each point in time, and each of those three models are gonna be pulled or evaluated on their objective criteria.
And so in this case, AUC starts at 50% and goes up to 78 and then a little bit higher.
So in that first pass sagemaker's training three models at a single point in time, it's gonna pull the results from that objective criteria.
And that is gonna go into a Bayesian Optimizer that's going to go into a Bayesian inferencer that's actually going to suggest the next best initializations for all of those hyperparameters.
And so that's that next point in time when three more models are getting initialized, and so you can say that on average that performance starts to go up, and that's where we're seeing an improvement on our model.
But then it starts to come down, and that's what our models actually overfitting.
And so that's where will typically apply regularization term or just use a smaller model to help prevent that overfitting, and so let's see it in action.
So over here on our console, this is from the sagemaker examples.
This is the hyperparameter tuning direct marketing with XGBoost, and essentially I am same as always just gonna hit run all on this.
Just so we can see it.
But the flow here is really similar, right?
We're gonna import sagemaker.
We've got the different types of tuners that we need.
we got our bucket and a roll.
This is still gonna be downloading a data set.
So this is from UCI and we're gonna take that data set, and then we're gonna load it into Pandas, of course.
So that's that pd dot read CSV.
So here's our data set and then over on the right hand side, that's our y column, Right.
That's the target.
And so in this case, this is direct marketing.
So these researchers called physical homes in Portugal and actually asked people if they would open a bank account or not most people overwhelmingly said no.
Some people said yes.
And so that's why you see the difference between no's and yes's.
And then our researchers are gonna put this into a model to see if they can figure out who is most likely to want to open a bank account.
And so some analysis on this right, they're gonna clean this data set up as we do in some capacity.
Ah, they're dropping some some terms here.
They're splitting it right right so that's that train validation and test then they're going to concat it.
So, we're putting that y Column First then they copy back to s3.
Here's the setup on the hyperparameter tuner, right?
So we're we've we're getting our image.
So that's where we're actually getting XGBoost, right?
It just lives in ECR so we've got our container.
There's our ec2 instance configs we're running on one m4 XL.
Here the hyperparameters that we're gonna set.
Here are those ranges right?
So eta, min child weight, alpha, etc.
Objective Metric.
That's that validation AUC, here's the tuner and then we call tuner dot fit and it's in progress.
Let's see it in the console.
So under tuning jobs.
Looks like we have a new tuning job that just started.
So that's our status and this is running.
So we have three models that were actually initializing, and so each of those models we can drill down into I'll Let's see the status on this, right?
So it's still just prepping the instances So still just getting going here.
So I'm gonna go to a previous tuning job that was done successfully, and I want to show you a single training job.
Just you can see it because there's a lot to know.
So that's our status on this previous job.
Download the input data, the training image complete in, and then we want to know how to check the logs on this.
So down here under monitor, we can hit view logs that's gonna port us over to cloudwatch.
And then cloudwatch is going to tell us that point by point read in time in terms of how well our model was performing.
And so, you know, for each point in time, this is our tree.
That's that's getting pruned by the booster along with our validation AUC, and it's it's performing and doing well.
And so what we can do is get the name of that tuning job, right?
So I'm just gonna copy the name of that job that ran previously I'm gonna cruise over here.
I actually need the name of that example, which is right over here.
So we're gonna check out sagemaker examples and then down here under hyperparameter tuning we're gonna analyze job results.
So let's hit use.
We'll create our copy.
And then just in this for self, I'll get rid of this and paste in the name of that tuning job.
Not that bad.
Cell run all okay.
And so this is going to tell us that for that hyperparameter tuning job, we ran 20 training jobs.
They've all completed our best model has a validation AUC of 77%.
Then we're actually gonna grep through cloudwatch logs and call this beautiful function tuner dot data frame.
And this is what tuner dot data frame is gonna give us.
Those these are the models that we've trained these are, all the 20 jobs that we ran Each of those is gonna vary based on the objective metric, the training time.
The hyperparameters are over here.
And then we can use this function that's built in, and then we just get a nice little chart, right?
And so this chart is just gonna tell us about how those models actually ran.
So here are those three models, along with their objective criteria and then another initialization and on and on.
And so again, sagemaker is using a bayesian optimizer You can also use random search in order to find the best models.
And so here we go.
And then you can also break it down based on the correlation between the changes in your hyperparameter and the changes in your objective criteria.
So a couple tips to close this out.
You need your s3 bucket to be in the same region as your training job.
Hard and fast min date.
You will run into errors if you don't do that.
Definitely used sagemaker search to compare the results of previous jobs that you ran.
So most commonly from starting a new project, I'm going to create a bucket with the name of that project.
And then I can use sagemaker search just to list all the training jobs that had an s3 uri with my bucket, and then that's gonna tell me the results of all those jobs Definitely run training jobs in parallel from your notebook, right?
So you don't have to wait for one job to have run previously, get a loop and then map reduce that loop through all cores and you can save a lot of time.
You can set the number of instances, so that's a soft limit.
You can set If you contact your team, your your account team, and then definitely check the documentation around which hyperparameters are valid for tuning.
And you can also use the tuner for bringing your own algorithm again.
Whether that's docker or script mode.
Just set your hyperparameters as a JSON object.
Thank you.
My name's Emily Webber.
I'm a machine learning special solutions architect at Amazon Web services.
Thank you for your time.
I hope you enjoyed learning about training and tuning jobs.
Definitely check out our github site.
Amazon SageMaker Examples for more information.
Thank you.
