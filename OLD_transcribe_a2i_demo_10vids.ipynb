{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Amazon Transcribe transcriptions using Custom Vocabularies and Amazon Augmented AI (A2I)\n",
    "\n",
    "\n",
    "\n",
    "This notebook accompanies the blog \"Improving Amazon Transcribe transcriptions using Custom Vocabularies and Amazon Augmented AI (A2I)\" (TODO: add link)\n",
    "\n",
    "## Introduction\n",
    "When transcribing speech containing domain-specific terminologies in fields such as legal, financial, construction, higher education, or engineering, Amazon Transcribe’s [custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/how-vocabulary.html) feature can improve transcription quality. \n",
    "\n",
    "To use custom vocabularies with Amazon Transcribe, you need a list of domain-specific terms. If you have a collection of videos or audio files (dataset) that you want transcribed with high accuracy, you can use a portion of your dataset to Amazon Transcribe to assess which terms it has difficulty with (low confidence predictions). You can use Amazon A2I to send these low-confidence predictions directly to a human to review and manually transcribe the terms. This walkthrough will demonstrate how you can process the results obtained from Amazon A2I to quickly to build a custom vocabulary.\n",
    "\n",
    "In summary, in this walkthrough you will:\n",
    "* Send a subset of videos to Amazon Transcribe to find terms that are difficult to transcribe.\n",
    "* Set up a human review workflow using Amazon A2I to send low-confidence predictions to humans for manual review and transcription.\n",
    "* Create a custom vocabulary using the results obtained from human workers.\n",
    "* Test Amazon Transcribe on another subset of videos to asses the improvement in transcription quality. \n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you can simply execute each cell in order. To understand what's happening, you'll need:\n",
    "\n",
    "* An S3 bucket you can write to -- please provide its name in BUCKET. The bucket must be in the same region as this SageMaker Notebook instance. You can also change the EXP_NAME to any valid S3 prefix. All the files related to this experiment will be stored in that prefix of your bucket.\n",
    "* Familiarity with the Amazon A2I.\n",
    "* Familiarity with Python and numpy.\n",
    "* Basic familiarity with AWS S3.\n",
    "* Basic understanding of Amazon Transcribe and custom vocabularies. \n",
    "* Basic familiarity with AWS Command Line Interface (CLI) -- ideally, you should have it set up with credentials to access the AWS account you're running this notebook from.\n",
    "\n",
    "This notebook has only been tested on a SageMaker notebook instance. The runtimes given are approximate. We used an ml.t2.medium instance in our tests. However, you can likely run it on a local instance by first executing the cell below on SageMaker and then copying the role string to your local copy of the notebook.\n",
    "\n",
    "For more sample notebooks using A2I, visit this [Github repository](https://github.com/aws-samples/amazon-a2i-sample-jupyter-notebooks).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Latest SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the latest installations of our dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install boto3 --upgrade\n",
    "!pip install -U botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import uuid\n",
    "import botocore\n",
    "import boto3\n",
    "import time\n",
    "import pprint\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Amazon SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Amazon Augment AI (A2I) client\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "\n",
    "# Amazon S3 (S3) client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Amazon Transcribe client\n",
    "transcribe = boto3.client(\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region, Bucket, and Paths\n",
    "Make sure all your resources are stored in the same region. You'll be using the same bucket for this entire walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'jashuang-sagemaker-5-22'\n",
    "EXP_NAME = '' # Any valid S3 prefix.\n",
    "OUTPUT_PATH = f's3://{BUCKET}/a2i-results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client('s3')\n",
    "bucket_region = s3.head_bucket(Bucket=BUCKET)['ResponseMetadata']['HTTPHeaders']['x-amz-bucket-region']\n",
    "assert bucket_region == region, \"Your S3 bucket {} and this notebook need to be in the same region.\".format(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following policies to this role in IAM:\n",
    "* AmazonAugmentedAIFullAccess\n",
    "* AmazonTranscribeFullAccess\n",
    "\n",
    "Or you can add a single policy, which will grant permissions to Amazon A2I and all integrated services (Amazon Rekognition and Amazon Transcribe)\n",
    "* AmazonAugmentedAIIntegratedAPIAccess\n",
    "\n",
    "Your execution role has the AmazonSageMakerFullAccess policy attached. This gives Amazon SageMaker permission to access your resources in S3 if the bucket or objects have the word `sagemaker` in the name. If your S3 bucket listed in `BUCKET` does not have sagemaker in the name, you will need to add an S3 policy to your execution role to give your role permissions to access your data objects in S3. The following is an example of an S3 policy:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::my_input_bucket/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::my_output_bucket/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::688520471316:role/service-role/AmazonSageMaker-ExecutionRole-20200522T134110'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "ROLE = get_execution_role()\n",
    "display(ROLE)\n",
    "\n",
    "# role_name = role.split('/')[-1]\n",
    "# print(f'Your execution role name: {role_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Sample Video to S3\n",
    "For this demo, we'll be analyzing an introductory video about SageMaker that's titled \"Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive\" and features Emily Webber, an ML Specialist at AWS.\n",
    "\n",
    "TODO: upload the video to a public blog bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Basic Transcription Job\n",
    "Our first step is to look at the performance of Amazon Transcribe using default parameters and establish a baseline for comparison. Once you have the SageMaker video mp4 file uploaded to an S3 bucket, you can use the transcribe function to start a transcription job. Note that the `vocab_name` parameter will be used later to specify custom vocabularies, and it’s currently defaulted to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AWS-sage-vid-0-21.05.44']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can give each transcription job any name. We attach a timestamp to each job name here\n",
    "# to prevent conflicting job names in case we need to re-run any jobs.\n",
    "now = datetime.now()\n",
    "time_now = now.strftime(\"%H.%M.%S\")\n",
    "\n",
    "# Path to folder\n",
    "folder_path = f\"s3://{BUCKET}/a2i_transcribe_demo/\"\n",
    "\n",
    "# Names of the video titles. If you want to test more videos, uncomment them below.\n",
    "all_videos = [\n",
    "             'Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive.mp4',\n",
    "#              'Built-in Machine Learning Algorithms with Amazon SageMaker - a Deep Dive.mp4',\n",
    "#              'Bring Your Own Custom ML Models with Amazon SageMaker.mp4',\n",
    "#              'Train Your ML Models Accurately with Amazon SageMaker.mp4',\n",
    "#              'Deploy Your ML Models to Production at Scale with Amazon SageMaker.mp4',\n",
    "#              'Tune Your ML Models to the Highest Accuracy with Amazon SageMaker Automatic Model Tuning.mp4',\n",
    "#              'Scale up Training of Your ML Models with Distributed Training on Amazon SageMaker.mp4',\n",
    "#              'Use the Deep Learning Framework of Your Choice with Amazon SageMaker.mp4',\n",
    "#              'Learn to Analyze the Co-Relation in Your Datasets Using Feature Engineering with Amazon SageMake.mp4',\n",
    "#              'Get Scheduled Predictions on Your ML Models with Amazon SageMaker Batch Transform.mp4'\n",
    "]\n",
    "\n",
    "num_videos = len(all_videos)\n",
    "\n",
    "job_names = []\n",
    "for i in range(num_videos):\n",
    "    job_names.append(\"AWS-sage-vid-\" + str(i) + \"-\" + str(time_now))\n",
    "\n",
    "job_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this demo, we'll be using the first and third videos as the in-sample videos for building our custom vocabulary. The second and fourth videos will be used as out-sample videos that we'll use to test Transcribe again once we've build the custom vocabulary. Feel free to include extra videos in these two sets depending on how many tasks you want to be created (more videos typically means more transcription errors and thus more human review tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the two in-sample videos\n",
    "insample_video_indices = [0,2]\n",
    "\n",
    "# Indices of the two out-sample videos\n",
    "outsample_video_indices = [1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a transcribe function\n",
    "def transcribe(job_name, job_uri, out_bucket, format=\"mp4\", vocab_name=None):\n",
    "    \"\"\"Transcribe a .wav or .mp4 file to text.\n",
    "    Args:\n",
    "        job_name (str): the name of the job that you specify;\n",
    "                        the output json will be job_name.json\n",
    "        job_uri (str): input path (in s3) to the file being transcribed\n",
    "        out_bucket (str): s3 bucket name that you want the output json\n",
    "                          to be placed in\n",
    "        format (str): mp4 or wav for input file format;\n",
    "                      defaults to mp4\n",
    "        vocab_name (str): name of custom vocabulary used;\n",
    "                          optional, defaults to None\n",
    "    \"\"\"\n",
    "    \n",
    "    if format not in ['mp3','mp4','wav','flac']:\n",
    "        print(\"Invalid format\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(\"------\" + format)\n",
    "        if vocab_name is None:\n",
    "            transcribe.start_transcription_job(\n",
    "                TranscriptionJobName=job_name,\n",
    "                Media={\"MediaFileUri\": job_uri},\n",
    "                MediaFormat=format,\n",
    "                LanguageCode=\"en-US\",\n",
    "                OutputBucketName=out_bucket,\n",
    "            )\n",
    "        else:\n",
    "            transcribe.start_transcription_job(\n",
    "                TranscriptionJobName=job_name,\n",
    "                Media={\"MediaFileUri\": job_uri},\n",
    "                MediaFormat=format,\n",
    "                LanguageCode=\"en-US\",\n",
    "                OutputBucketName=out_bucket,\n",
    "                Settings={'VocabularyName': vocab_name}\n",
    "            )\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        print(transcribe.get_transcription_job(TranscriptionJobName=job_name))\n",
    "        \n",
    "#         while True:\n",
    "#             status = transcribe.get_transcription_job(TranscriptionJobName=job_name)\n",
    "#             if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "#                 break\n",
    "#             print(\"Not ready yet...\")\n",
    "#             time.sleep(5)\n",
    "#         print(status)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start a transcription job\n",
    "# transcribe(job_names[8], folder_path+all_videos[8], BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------mp4\n",
      "{'TranscriptionJob': {'TranscriptionJobName': 'AWS-sage-vid-0-23.46.55', 'TranscriptionJobStatus': 'IN_PROGRESS', 'LanguageCode': 'en-US', 'MediaSampleRateHertz': 44100, 'MediaFormat': 'mp4', 'Media': {'MediaFileUri': 's3://jashuang-sagemaker-5-22/transcribe-bucket/Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive.mp4'}, 'Transcript': {}, 'StartTime': datetime.datetime(2020, 6, 17, 23, 47, 12, 346000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 17, 23, 47, 12, 325000, tzinfo=tzlocal()), 'Settings': {'ChannelIdentification': False, 'ShowAlternatives': False}}, 'ResponseMetadata': {'RequestId': 'b9726bf8-4cd5-4d12-9104-e3c5f6d99f44', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Wed, 17 Jun 2020 23:47:14 GMT', 'x-amzn-requestid': 'b9726bf8-4cd5-4d12-9104-e3c5f6d99f44', 'content-length': '474', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "------mp4\n",
      "{'TranscriptionJob': {'TranscriptionJobName': 'AWS-sage-vid-1-23.46.55', 'TranscriptionJobStatus': 'IN_PROGRESS', 'LanguageCode': 'en-US', 'MediaSampleRateHertz': 44100, 'MediaFormat': 'mp4', 'Media': {'MediaFileUri': 's3://jashuang-sagemaker-5-22/transcribe-bucket/Built-in Machine Learning Algorithms with Amazon SageMaker - a Deep Dive.mp4'}, 'Transcript': {}, 'StartTime': datetime.datetime(2020, 6, 17, 23, 47, 14, 590000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 17, 23, 47, 14, 553000, tzinfo=tzlocal()), 'Settings': {'ChannelIdentification': False, 'ShowAlternatives': False}}, 'ResponseMetadata': {'RequestId': 'ef31137a-0036-4715-bb0c-7e37c5812e56', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Wed, 17 Jun 2020 23:47:16 GMT', 'x-amzn-requestid': 'ef31137a-0036-4715-bb0c-7e37c5812e56', 'content-length': '477', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "------mp4\n",
      "{'TranscriptionJob': {'TranscriptionJobName': 'AWS-sage-vid-2-23.46.55', 'TranscriptionJobStatus': 'IN_PROGRESS', 'LanguageCode': 'en-US', 'MediaSampleRateHertz': 44100, 'MediaFormat': 'mp4', 'Media': {'MediaFileUri': 's3://jashuang-sagemaker-5-22/transcribe-bucket/Bring Your Own Custom ML Models with Amazon SageMaker.mp4'}, 'Transcript': {}, 'StartTime': datetime.datetime(2020, 6, 17, 23, 47, 16, 939000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 17, 23, 47, 16, 913000, tzinfo=tzlocal()), 'Settings': {'ChannelIdentification': False, 'ShowAlternatives': False}}, 'ResponseMetadata': {'RequestId': 'bced3d1c-1509-466e-9374-ebac3512ab14', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Wed, 17 Jun 2020 23:47:18 GMT', 'x-amzn-requestid': 'bced3d1c-1509-466e-9374-ebac3512ab14', 'content-length': '459', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "------mp4\n",
      "{'TranscriptionJob': {'TranscriptionJobName': 'AWS-sage-vid-3-23.46.55', 'TranscriptionJobStatus': 'IN_PROGRESS', 'LanguageCode': 'en-US', 'MediaSampleRateHertz': 44100, 'MediaFormat': 'mp4', 'Media': {'MediaFileUri': 's3://jashuang-sagemaker-5-22/transcribe-bucket/Train Your ML Models Accurately with Amazon SageMaker.mp4'}, 'Transcript': {}, 'StartTime': datetime.datetime(2020, 6, 17, 23, 47, 19, 209000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 17, 23, 47, 19, 187000, tzinfo=tzlocal()), 'Settings': {'ChannelIdentification': False, 'ShowAlternatives': False}}, 'ResponseMetadata': {'RequestId': '8ac41630-2329-403b-b843-54301bcaab1c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Wed, 17 Jun 2020 23:47:20 GMT', 'x-amzn-requestid': '8ac41630-2329-403b-b843-54301bcaab1c', 'content-length': '459', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,4):\n",
    "    transcribe(job_names[i], folder_path+all_videos[i], BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check transcription job statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "for job_name in job_names:\n",
    "    print(transcribe.get_transcription_job(TranscriptionJobName=job_name)['TranscriptionJob']['TranscriptionJobStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and Parse Transcription Results\n",
    "\n",
    "When the transcription job finishes, the results will be stored in your specified S3 bucket as an output JSON file called “YOUR_JOB_NAME.json.” You can use the following function to retrieve your results, and parse them into sentences with time stamps, confidence scores, and other useful representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript_text_and_timestamps(bucket_name, file_name):\n",
    "    \"\"\"take json file from S3 bucket and returns a tuple of:\n",
    "       entire transcript, list object of tuples of timestamp and individual sentences\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): name of s3 bucket\n",
    "        file_name (str): name of file\n",
    "    Returns:\n",
    "        (\n",
    "        entire_transcript: str,\n",
    "        sentences_and_times: [ {start_time (sec) : float,\n",
    "                                end_time (sec)   : float,\n",
    "                                sentence         : str,\n",
    "                                min_confidence   : float (minimum confidence score of that sentence)\n",
    "                                } ],\n",
    "        confidences:  [ {start_time (sec) : float,\n",
    "                         end_time (sec)   : float,\n",
    "                         content          : str, (single word/phrase)\n",
    "                         confidence       : float (confidence score of the word/phrase)\n",
    "                         } ],\n",
    "        scores: list of confidence scores\n",
    "        )\n",
    "    \"\"\"\n",
    "    s3_clientobj = s3.get_object(Bucket=bucket_name, Key=file_name)\n",
    "    s3_clientdata = s3_clientobj[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "    original = json.loads(s3_clientdata)\n",
    "    items = original[\"results\"][\"items\"]\n",
    "    entire_transcript = original[\"results\"][\"transcripts\"]\n",
    "\n",
    "    sentences_and_times = []\n",
    "    temp_sentence = \"\"\n",
    "    temp_start_time = 0\n",
    "    temp_min_confidence = 1.0\n",
    "    newSentence = True\n",
    "    \n",
    "    confidences = []\n",
    "    scores = []\n",
    "\n",
    "    i = 0\n",
    "    for item in items:\n",
    "        # always add the word\n",
    "        if item[\"type\"] == \"punctuation\":\n",
    "            temp_sentence = (\n",
    "                temp_sentence.strip() + item[\"alternatives\"][0][\"content\"] + \" \"\n",
    "            )\n",
    "        else:\n",
    "            temp_sentence = temp_sentence + item[\"alternatives\"][0][\"content\"] + \" \"\n",
    "            temp_min_confidence = min(temp_min_confidence,\n",
    "                                      float(item[\"alternatives\"][0][\"confidence\"]))\n",
    "            confidences.append({\"start_time\": float(item[\"start_time\"]),\n",
    "                                \"end_time\": float(item[\"end_time\"]),\n",
    "                                \"content\": item[\"alternatives\"][0][\"content\"],\n",
    "                                \"confidence\": float(item[\"alternatives\"][0][\"confidence\"])\n",
    "                               })\n",
    "            scores.append(float(item[\"alternatives\"][0][\"confidence\"]))\n",
    "\n",
    "        # if this is a new sentence, and it starts with a word, save the time\n",
    "        if newSentence == True:\n",
    "            if item[\"type\"] == \"pronunciation\":\n",
    "                temp_start_time = float(item[\"start_time\"])\n",
    "            newSentence = False\n",
    "        # else, keep going until you hit a punctuation\n",
    "        else:\n",
    "            if (\n",
    "                item[\"type\"] == \"punctuation\"\n",
    "                and item[\"alternatives\"][0][\"content\"] != \",\"\n",
    "            ):\n",
    "                # end time of sentence is end_time of previous word\n",
    "                end_time = items[i-1][\"end_time\"] if i-1 >= 0 else items[0][\"end_time\"]\n",
    "                sentences_and_times.append(\n",
    "                    {\"start_time\": temp_start_time,\n",
    "                     \"end_time\": end_time,\n",
    "                     \"sentence\": temp_sentence.strip(),\n",
    "                     \"min_confidence\": temp_min_confidence\n",
    "                    }\n",
    "                )\n",
    "                # reset the temp sentence and relevant variables\n",
    "                newSentence = True\n",
    "                temp_sentence = \"\"\n",
    "                temp_min_confidence = 1.0\n",
    "                \n",
    "        i = i + 1\n",
    "        \n",
    "    sentences_and_times.append(\n",
    "                    {\"start_time\": temp_start_time,\n",
    "                     \"end_time\": confidences[-1][\"end_time\"],\n",
    "                     \"sentence\": temp_sentence.strip(),\n",
    "                     \"min_confidence\": temp_min_confidence\n",
    "                    }\n",
    "                )\n",
    "    return entire_transcript, sentences_and_times, confidences, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire_transcript_1, sentences_and_times_1, confidences_1, scores_1 = get_transcript_text_and_timestamps(\"jashuang-sagemaker-5-22\",\"AWS-sage-vid-3.json\")\n",
    "\n",
    "# print(entire_transcript_1)\n",
    "\n",
    "# file0 = open(\"ground_truth_4.txt\",\"w\") \n",
    "# for tup in sentences_and_times_1:\n",
    "#     file0.write(tup['sentence'] + \"\\n\") \n",
    "# file0.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entire_transcript = []\n",
    "all_sentences_and_times = []\n",
    "all_confidences = []\n",
    "all_scores = []\n",
    "for i in range(0,4):\n",
    "    entire_transcript_1, sentences_and_times_1, confidences_1, scores_1 = get_transcript_text_and_timestamps(\"jashuang-sagemaker-5-22\",job_names[i]+\".json\")\n",
    "    all_entire_transcript.append(entire_transcript_1)\n",
    "    all_sentences_and_times.append(sentences_and_times_1)\n",
    "    all_confidences.append(confidences_1)\n",
    "    all_scores.append(scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_time': 499.58, 'end_time': '510.65', 'sentence': \"So it's either python and three or two or are if you've installed it or in a condo or any of the other capabilities that you're gonna need and all those, they're gonna come with your sagemaker notebook instance.\", 'min_confidence': 0.5654}\n"
     ]
    }
   ],
   "source": [
    "# Sanity check!\n",
    "print(all_sentences_and_times[0][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_scores_list = all_scores[0] + all_scores[2] #[item for sublist in all_scores[0,2] for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the original transcripts to txt files\n",
    "Let's save the full transcripts, as we'll be using this later for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for sentences_times in all_sentences_and_times:\n",
    "    file0 = open(f\"original_transcript_{i}.txt\",\"w\") \n",
    "    for tup in sentences_times:\n",
    "        file0.write(tup['sentence'] + \"\\n\") \n",
    "    file0.close()\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of confidence scores\n",
    "Let’s take a look at the distribution of confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.xlim([min(flat_scores_list)-0.1, max(flat_scores_list)+0.1])\n",
    "plt.hist(flat_scores_list, bins=20, alpha=0.5)\n",
    "plt.title('Plot of confidence scores')\n",
    "plt.xlabel('Confidence score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of low confidence scores\n",
    "Let’s filter out the high confidence scores to take a closer look at the lower ones.\n",
    "You can experiment with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 words that have confidence score less than 0.35\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.35\n",
    "\n",
    "# Filter scores that are less than THRESHOLD\n",
    "all_bad_scores = [i for i in flat_scores_list if i < THRESHOLD]\n",
    "print(f\"There are {len(all_bad_scores)} words that have confidence score less than {THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAG8tJREFUeJzt3XmYJWV99vHvzbAvIjrjwjogi47GKIwao7hENGAUTDAK0USMkcQtC1nE5UVf1BhjIup7qYhLiBsIRAlREsQIaowIgyIwIDoCyuIyIAiIQsDf+0c9XRw6vZzp6dM93fP9XFddXetTT9Wprru2UydVhSRJAJvMdwUkSRsOQ0GS1DMUJEk9Q0GS1DMUJEk9Q0GS1DMUNnBJzk3yR3M0r5cl+VGS25Lcf47m+cAkX0pya5J/TPLaJB+cYvyrkxwwF3VbbJJUkj3ncf5+dguAobABaP8sP2874x8lOTHJtutYxvL2T7/pDOuwGfAO4BlVtW1V3TiTcmbgSOAG4D5V9ZdV9bdVNSchqNFp2/Cb53H+T0vyrSS3JzknyW6TjPeAJCcluT7JT5N8Jcnj5rq+GxJDYcPx7KraFtgXWAm8fo7n/0BgS2D1HM93N+Cy8luUE0qyZL7rsNAkWQp8Cvg/wP2AVcAnJxl9W+ACYL827j8Dn13Xg7JFpaps5rkBrgYOGOh+O/CZ1n4u8EetfRO6sPge8GPgI8D2bdj3gQJua83jJ5jPFsA7getb887Wb2/gZwPTf2GSej4R+G/gZuAa4IjWf/tWl7Wtbq8HNmnDjgD+C/gH4CbgKuCgNuxE4H+AO9t8DwDeCHxsYJ6/38q8EXjd4Lpq6+No4Ltt+CnA/dqw5W15XtTWzQ3A6wbKXQK8tk17K3AhsEsb9lDgbOAnwBXA86b47I4ArmxlXAW8YGDYS4HL27DLgH1b/4e1z/VmuhA+eGCaE4H3AWe2z+SA9hn9Q1uOHwHHA1u18ZcCn2ll/QT48ti6n6CuBew5sC2sc5nAq4Hr2jJdATxtgvkcOe5z/beB7fyvgIuBn9LtqLdsw3Zo81xLt518Bth5oMxzgTcBX2nz/hywdJLlPBL474HubYCfAw8d8v/xFmC/+d4vzFcz7xWwuXcoALu0HcWbWve53BMKfwisAfagO8L5FPDRNmx5+6ffdIr5HAucBzwAWEa3g3/TMNPTHdHfChwObAbcH3hUG/YR4F+B7Vo53wZe0oYd0XYQL6XbEb+MLpDShp8IvHlgPm+khQKwou1UnkS3E3sHcNfAuvqztjw7t+HvB04atzwfALYCfhW4A3hYG/7XwCXAPkDa8Pu3Hcg1wIuBTYFH0wXKignWyTZtB7JP634w8PDW/rt0O8/HtPL3bOtws/YZvhbYHPiNtl73GVgfPwWeQBd6WwLHAWfQHcluB/wb8NY2/lvpduibtWb/sXU7QX0HQ2Gdy2zr6hpgx4F1/JBJ5nWvz3VgOz8f2LHN93LgT9qw+wOHAlu3+pwKnD4w7bl0Ab53+zzPBf5uknm/C3jfuH6XAocO8b/4KOAXtIOtjbGZ9wrY9P8st9EdmX0PeC/3HLWdyz2h8J/Aywem24duh7spw4XCd4FnDnT/JnB1a59yeuA1wKcn6L+E7ohwxUC/PwbObe1HAGsGhm3d5vOg1n2vnQf3DoVjgJMHhm3T5jUWCpczcKRKt1Mevz4GjzbPBw5r7VcAh0ywPM8Hvjyu3/uBN0ww7jbtMzt07PMaGHYW8GcTTLM/8EMGjuaBk4A3DqyPjwwMC90Zw0MG+j0euKq1H0sXyHsOsZ0VXTjNqMw27Y/pzl42m2Ze9/pcB7bzFw50/z1w/CTTPwq4aaD7XOD1A90vB/5jkmk/xLjAoDvDOGKaOt+H7kDhNcP+7y7GxnsKG47nVNV9q2q3qnp5Vf18gnF2pAuNMd+j2wE+cMh5TDT9jkNOuwtdqIy3lO5ocny5Ow10/3Cspapub63DXLPdke7IdGzan9FdJhqzG/DpJDcnuZkuJO7m3uvjhwPttw/Md7Ll2Q143FiZrdwXAA8aP2Krz/OBPwF+kOSzSR46Tfk7AtdU1S8H+o1fX9cMtC+jC9ILB+rzH60/dJca1wCfS3JlkqMnmOd4MyqzqtYAf04X3D9OcnKSYbefMRN+Hkm2TvL+JN9LcgvwJeC+4+6pTPZZjncb3Q5+0H3ozsgmlGQrurOl86rqrUMtySJlKCws19PttMbsSnc55Ud0R4Ezmf76Ied9DfCQCfrfQHd0Pr7c64Ysdyo/oNu5At2Og+4yw2CdDmphOtZsWVXDzHuy5bkG+OK4MretqpdNVEhVnVVVT6c7S/kW3eWqqcq/HtglyeD/3vj1NfhZ3kB3PfzhA/XZvrqHEqiqW6t7amsP4GDgqCRPm2bZZ1xmVX2iqp5I93kX8LZJ5jHM9jjoL+nOfB9XVfehu2QI3VnNulpNdzmwKyDZhu6zmPAhiiRbAKcD19Kd5W7UDIWF5STgL5Ls3p6O+Fvgk1V1F90Nul/S3W+YavrXJ1nWntA4BvjYkPP+OHBAkucl2TTJ/ZM8qqruprvB+5Yk27VH/45ah3KnchrwrCRPTLI53WWNwW32+Dbf3QDach0yZNkfBN6UZK90Htm+m/EZYO8kv59ks9Y8JsnDxhfQvmNxSNvp3EF3hPrLgfL/Ksl+rfw9Wz2/RneU+zet7KcAzwZOnqiS7YziA8BxSR7Q5rtTkt9s7c9qZYfuXsTdA3WY0EzLTLJPkt9oO9Ff0AXLZPP6EVNvi+Nt18q7Ocn9gDesw7TjfRp4RJJDk2xJt51fXFXfGj9iexT7tDbvF407g9soGQoLy4eBj9KdWl9F94/5Kugvy7wF+Eq7JPBrE0z/ZrrH8y6mu3b69dZvWlX1feCZdEd0PwEu4p6jsVfRXaO+ku5Jo0+0uq6XqloNvKKV9wO6p1KuHRjlXXQ3Sz+X5Fa6m87DPmP+Drow+xzdzeIP0d0XuBV4BnAY3VH9D+mOhreYoIxN6ALwerp18mS6G+lU1al0n8cn6C5bnE73ZNSddCFwEN0R+3uBP5hohzXg1XSXc85rl1Y+T3dUDbBX674N+Crw3qo6Z4jln0mZWwB/1+r9Q7oHFl4zSfkfAla0bfH0IerzTrobyDfQfY7/McQ0E6qqtXT3ed5Ct808ju7zBCDJ8UmOb52/DjyL7jO/uX1X6LYk+890/gvd2BMgkiR5piBJuoehIEnqGQqSpJ6hIEnqzeiNmvNp6dKltXz58vmuhiQtKBdeeOENVbVsuvEWXCgsX76cVatWzXc1JGlBSfK96cfy8pEkaYChIEnqGQqSpJ6hIEnqGQqSpJ6hIEnqjSwUknw4yY+TXDrJ8CR5d5I1SS5Osu+o6iJJGs4ozxROBA6cYvhBdK/o3Yvuh7bfN8K6SJKGMLJQqKov0b1jfjKH0P0WbVXVeXQ/vffgUdVHkjS9+fxG807c+7dor239fjB+xCRH0p1NsOuuu85J5bRhO+7sb/ftf/H0veexJhuGwfUxnutH62JB3GiuqhOqamVVrVy2bNpXd0iSZmg+Q+E6Bn6UHdiZ2fmxd0nSDM1nKJwB/EF7CunXgJ9W1f+6dCRJmjsju6eQ5CTgKcDSJNcCbwA2A6iq44Ez6X4Ifg1wO/DiUdVFkjSckYVCVR0+zfACXjGq+UuS1t2CuNEsSZobhoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqTfSUEhyYJIrkqxJcvQEw3dNck6SbyS5OMkzR1kfSdLURhYKSZYA7wEOAlYAhydZMW601wOnVNWjgcOA946qPpKk6Y3yTOGxwJqqurKq7gROBg4ZN04B92nt2wPXj7A+kqRpjDIUdgKuGei+tvUb9EbghUmuBc4EXjVRQUmOTLIqyaq1a9eOoq6SJOb/RvPhwIlVtTPwTOCjSf5XnarqhKpaWVUrly1bNueVlKSNxShD4Tpgl4HunVu/QS8BTgGoqq8CWwJLR1gnSdIURhkKFwB7Jdk9yeZ0N5LPGDfO94GnASR5GF0oeH1IkubJyEKhqu4CXgmcBVxO95TR6iTHJjm4jfaXwEuTfBM4CTiiqmpUdZIkTW3TURZeVWfS3UAe7HfMQPtlwBNGWQdJ0vDm+0azJGkDYihIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpN1QoJPmVUVdEkjT/hj1TeG+S85O8PMn2I62RJGneDBUKVbU/8AJgF+DCJJ9I8vSR1kySNOeGvqdQVd8BXg+8Gngy8O4k30ryO6OqnCRpbg17T+GRSY4DLgd+A3h2VT2stR83wvpJkubQsGcK/w/4OvCrVfWKqvo6QFVdT3f2MKEkBya5IsmaJEdPMs7zklyWZHWST6zrAkiSZs+mQ473W8DPq+pugCSbAFtW1e1V9dGJJkiyBHgP8HTgWuCCJGdU1WUD4+wFvAZ4QlXdlOQB67EskqT1NOyZwueBrQa6t279pvJYYE1VXVlVdwInA4eMG+elwHuq6iaAqvrxkPWRJI3AsKGwZVXdNtbR2reeZpqdgGsGuq9t/QbtDeyd5CtJzkty4JD1kSSNwLCh8LMk+451JNkP+PkszH9TYC/gKcDhwAeS3Hf8SEmOTLIqyaq1a9fOwmwlSRMZ9p7CnwOnJrkeCPAg4PnTTHMd3fcaxuzc+g26FvhaVf0PcFWSb9OFxAWDI1XVCcAJACtXrqwh6yxJWkdDhUJVXZDkocA+rdcVbUc+lQuAvZLsThcGhwG/N26c0+nOEP4pyVK6y0lXDlt5SdLsGvZMAeAxwPI2zb5JqKqPTDZyVd2V5JXAWcAS4MNVtTrJscCqqjqjDXtGksuAu4G/rqobZ7gskqT1NFQoJPko8BDgIrqdN0ABk4YCQFWdCZw5rt8xA+0FHNUaSdI8G/ZMYSWwou3EJUmL1LBPH11Kd3NZkrSIDXumsBS4LMn5wB1jPavq4JHUSpI0L4YNhTeOshKSpA3DsI+kfjHJbsBeVfX5JFvTPVEkSVpEhn119kuB04D3t1470X3HQJK0iAx7o/kVwBOAW6D/wR3faCpJi8ywoXBHe9MpAEk2pfuegiRpERk2FL6Y5LXAVu23mU8F/m101ZIkzYdhQ+FoYC1wCfDHdN9SnvQX1yRJC9OwTx/9EvhAayRJi9Sw7z66ignuIVTVHrNeI0nSvFmXdx+N2RL4XeB+s18dSdJ8GuqeQlXdONBcV1XvBH5rxHWTJM2xYS8f7TvQuQndmcO6/BaDJGkBGHbH/o8D7XcBVwPPm/XaSJLm1bBPHz111BWRJM2/YS8fTfnLaFX1jtmpjiRpPq3L00ePAc5o3c8Gzge+M4pKSZLmx7ChsDOwb1XdCpDkjcBnq+qFo6qYJGnuDfuaiwcCdw5039n6SZIWkWHPFD4CnJ/k0637OcA/j6ZKkqT5MuzTR29J8u/A/q3Xi6vqG6OrliRpPgx7+Qhga+CWqnoXcG2S3UdUJ0nSPBn25zjfALwaeE3rtRnwsVFVSpI0P4Y9U/ht4GDgZwBVdT2w3agqJUmaH8OGwp1VVbTXZyfZZnRVkiTNl2FD4ZQk7wfum+SlwOfxB3ckadEZ9umjf2i/zXwLsA9wTFWdPdKaSZLm3LShkGQJ8Pn2UjyDQJIWsWkvH1XV3cAvk2w/B/WRJM2jYb/RfBtwSZKzaU8gAVTVn46kVpKkeTFsKHyqNZKkRWzKUEiya1V9v6pm9J6jJAcC7wKWAB+sqr+bZLxDgdOAx1TVqpnMS5K0/qa7p3D6WEuSf1mXgtsN6vcABwErgMOTrJhgvO2APwO+ti7lS5Jm33ShkIH2Pdax7McCa6rqyqq6EzgZOGSC8d4EvA34xTqWL0maZdOFQk3SPoydgGsGuq9t/XpJ9gV2qarPTlVQkiOTrEqyau3atetYDUnSsKa70fyrSW6hO2PYqrXTuquq7jPTGSfZBHgHcMR041bVCcAJACtXrlzXcJIkDWnKUKiqJetR9nXALgPdO7d+Y7YDHgGcmwTgQcAZSQ72ZrMkzY91+T2FdXUBsFeS3ZNsDhwGnDE2sKp+WlVLq2p5VS0HzgMMBEmaRyMLhaq6C3glcBZwOXBKVa1OcmySg0c1X0nSzA375bUZqaozgTPH9TtmknGfMsq6SJKmN8rLR5KkBcZQkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1RhoKSQ5MckWSNUmOnmD4UUkuS3Jxkv9Mstso6yNJmtrIQiHJEuA9wEHACuDwJCvGjfYNYGVVPRI4Dfj7UdVHkjS9UZ4pPBZYU1VXVtWdwMnAIYMjVNU5VXV76zwP2HmE9ZEkTWOUobATcM1A97Wt32ReAvz7RAOSHJlkVZJVa9euncUqSpIGbRA3mpO8EFgJvH2i4VV1QlWtrKqVy5Ytm9vKSdJGZNMRln0dsMtA986t370kOQB4HfDkqrpjhPWRJE1jlGcKFwB7Jdk9yebAYcAZgyMkeTTwfuDgqvrxCOsiSRrCyEKhqu4CXgmcBVwOnFJVq5Mcm+TgNtrbgW2BU5NclOSMSYqTJM2BUV4+oqrOBM4c1++YgfYDRjl/SdK62SBuNEuSNgyGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknojDYUkBya5IsmaJEdPMHyLJJ9sw7+WZPko6yNJmtrIQiHJEuA9wEHACuDwJCvGjfYS4Kaq2hM4DnjbqOojSZreKM8UHgusqaorq+pO4GTgkHHjHAL8c2s/DXhakoywTpKkKWw6wrJ3Aq4Z6L4WeNxk41TVXUl+CtwfuGFwpCRHAke2ztuSXLEO9Vg6vryN0KJeB0cNN9qiXgdTGVg/G+06aDb25d9tmJFGGQqzpqpOAE6YybRJVlXVylmu0oLiOnAdgOtgY1/+YY3y8tF1wC4D3Tu3fhOOk2RTYHvgxhHWSZI0hVGGwgXAXkl2T7I5cBhwxrhxzgBe1NqfC3yhqmqEdZIkTWFkl4/aPYJXAmcBS4APV9XqJMcCq6rqDOBDwEeTrAF+Qhccs21Gl50WGdeB6wBcBxv78g8lHphLksb4jWZJUs9QkCT1FnQozPQ1GkmWJ/l5kotac/xc1322DLEOnpTk60nuSvLcccNelOQ7rXnR+GkXgvVc/rsHtoHxD0EsGEOsg6OSXJbk4iT/mWS3gWELfhuA9V4Hi2I7mDVVtSAbupvX3wX2ADYHvgmsGDfOy4HjW/thwCdb+3Lg0vlehjlaB8uBRwIfAZ470P9+wJXt7w6tfYf5Xqa5Wv427Lb5XoY5WgdPBbZu7S8b+D9Y8NvA+q6DxbIdzGazkM8UfI3GEOugqq6uqouBX46b9jeBs6vqJ1V1E3A2cOBcVHoWrc/yLxbDrINzqur21nke3XeGYHFsA7B+60DjLORQmOg1GjtNNk5V3QWMvUYDYPck30jyxST7j7qyIzLMOhjFtBuK9V2GLZOsSnJekufMbtXmzLqug5cA/z7DaTdU67MOYHFsB7NmQbzmYgR+AOxaVTcm2Q84PcnDq+qW+a6Y5tRuVXVdkj2ALyS5pKq+O9+VGpUkLwRWAk+e77rMl0nWwUa1HUxnIZ8pzPg1GlV1R1XdCFBVF9Jdj9x75DWefcOsg1FMu6FYr2Woquva3yuBc4FHz2bl5shQ6yDJAcDrgIOr6o51mXYBWJ91sFi2g9kz3zc1ZtrQneVcCezOPTeXHj5unFdw7xvNp7T2ZcCS1r4H3QZ0v/leplGsg4FxT+R/32i+iu4G4w6tfUGtg/Vc/h2ALVr7UuA7jLs5uRCaIf8PHk134LPXuP4LfhuYhXWwKLaDWV2f812B9dwYngl8u33Yr2v9jqU7EgDYEjgVWAOcD+zR+h8KrAYuAr4OPHu+l2WE6+AxdNdYf0b3ssHVA9P+YVs3a4AXz/eyzOXyA78OXNJ2IJcAL5nvZRnhOvg88KO2vV8EnLGYtoH1WQeLaTuYrcbXXEiSegv5noIkaZYZCpKknqEgSeoZCpKknqEgSeoZCtrgJXlQkpOTfDfJhUnOTDKjLxsm2T/J6vZGzJ2SnDbJeOcm8UfetdExFLRBay8w/DRwblU9pKr2A14DPHCGRb4AeGtVPaqqrquq5047xQKWZMl810ELi6GgDd1Tgf+pqv43L6rqm1X15XTenuTSJJckeT5Akqe0I/3TknwrycfbuH8EPA94U+u3PMmlbZqt2tnI5Uk+DWw1Nr8kz0jy1fa7DKcm2bb1vzrJ/239L0ny0NZ/2yT/1PpdnOTQqcoZlORPB977f/I05R3e+l2a5G0DZdyW5B+TfBN4fJL92osfL0xyVpIHz/JnpMVkvr89Z2MzVQP8KXDcJMMOpXvd8xK6M4fvAw8GnkL3Rtyd6Q58vgo8sU1zIu11Fwz8rgZwFPDh1v5I4C66F6ctBb4EbNOGvRo4prVfDbyqtb8c+GBrfxvwzoF67jBVOeOW6Xruee3Cfacob8e2vMvoXvPwBeA5bXgBz2vtmwH/DSxr3c8fW04bm4majfUtqVocngicVFV3Az9K8kW611rcApxfVdcCJLmILgD+a4qyngS8G6CqLk5ycev/a8AK4Cvtpzg2pwuZMZ9qfy8Efqe1H0D3ri1aeTcledY05Yy5GPh4ktOB06co70l0l9TWtmX8eFuG04G7gX9po+8DPAI4u813Cd1bgqUJGQra0K0GZnLd/46B9ruZ+bYeuh+iOXya+Uw3j+nKGfNbdDv3ZwOvS/Ir61LZ5hctKMfmu7qqHj+DcrQR8p6CNnRfALZIcuRYjySPbD+M9GXg+UmWJFlGtzM9f4bz+RLwe638R9BdQoLuV7qekGTPNmybIZ58OpvuDb1j9d1hmHKSbALsUlXn0F1e2h7YdpLyzgeenGRpu5l8OPDFCepyBbAsyePbtJslefh0K0MbL0NBG7SqKuC3gQPaI6mrgbcCP6R7KuliujdcfgH4m6r64Qxn9T5g2ySX071d88I2/7XAEcBJ7ZLSV4GHTlPWm4Ed2g3gbwJPHbKcJcDHklwCfAN4d1XdPEl5PwCOBs5py39hVf3r+IpU9/OUzwXe1qa9iO7NoNKEfEuqJKnnmYIkqWcoSJJ6hoIkqWcoSJJ6hoIkqWcoSJJ6hoIkqff/AdvSuYakTgsKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlim([min(all_bad_scores)-0.1, max(all_bad_scores)+0.1])\n",
    "plt.hist(all_bad_scores, bins=20, alpha=0.5)\n",
    "plt.title('Plot of confidence scores less than ' + str(threshold))\n",
    "plt.xlabel('Confidence score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.xlim([min(flat_scores_list)-0.1, max(flat_scores_list)+0.1])\n",
    "# plt.hist(flat_scores_list, bins=20, alpha=0.5)\n",
    "# plt.title('Plot of confidence scores')\n",
    "# plt.xlabel('Confidence score')\n",
    "# plt.ylabel('Frequency')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a nontrivial number of words classified with low confidence. As we’ll see later, technical terms are more often mis-transcribed, so it’s important that we correct those mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Human Review Workflow with A2I\n",
    "\n",
    "Our next step is create a human review workflow that sends low confidence scores to human reviewers and then retrieves the corrected transcription they provide. This section contains the following steps:\n",
    "\n",
    "1. Create a work task template that will be displayed to workers for every task. The template will be rendered with input data you provide, instructions to workers, and interactive tools to help workers complete your tasks.\n",
    "2. Create a human review workflow, also called a flow definition. You use the flow definition to configure details about your human workforce and the human tasks they are assigned.\n",
    "3. Create a human loop to start the human review workflow, sending data for human review as needed. In this example, you use a custom task type and start human loop tasks using the [Amazon A2I Runtime API](https://docs.aws.amazon.com/augmented-ai/2019-11-07/APIReference/Welcome.html). Each time StartHumanLoop is called, a task is sent to human reviewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workteam or Workforce\n",
    "\n",
    "\n",
    "A workforce is the group of workers that you have selected to label your dataset. You can choose either the Amazon Mechanical Turk workforce, a vendor-managed workforce, or you can create your own private workforce for human reviews. Whichever workforce type you choose, Amazon Augmented AI takes care of sending tasks to workers.\n",
    "\n",
    "When you use a private workforce, you also create work teams, a group of workers from your workforce that are assigned to Amazon Augmented AI human review tasks. You can have multiple work teams and can assign one or more work teams to each job.\n",
    "\n",
    "To create your Workteam, visit the instructions [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html).\n",
    "\n",
    "After you have created your workteam, replace YOUR_WORKTEAM_ARN below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKTEAM_ARN= \"arn:aws:sagemaker:us-west-2:688520471316:workteam/private-crowd/jashuang-test-workforce\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clients\n",
    "Let's setup the rest of our clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Amazon SageMaker client\n",
    "sagemaker = boto3.client('sagemaker', region)\n",
    "\n",
    "# Amazon Augment AI (A2I) client\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "\n",
    "s3 = boto3.client('s3', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Control Plane Resources\n",
    "Now let's create the resources we'll need to build our human review workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Human Task UI\n",
    "\n",
    "Amazon A2I uses Liquid, an open-source template language that can be used to “inject” data dynamically into HTML files.\n",
    "\n",
    "In this walkthrough, we want for each task to enable a human reviewer to watch a section of the video and transcribe the speech they hear. The HTML template consists of three main parts:\n",
    "\n",
    "1. A video player with a replay button that only allows the reviewer to play the specific subsection\n",
    "2. A form for the reviewer to type and submit what they hear\n",
    "3. Logic written in JavaScript to give the replay button its intended functionality\n",
    "\n",
    "For over 60 other pre-built UIs, check out this [repository](https://github.com/aws-samples/amazon-a2i-sample-task-uis).\n",
    "\n",
    "Here’s the template you’ll be using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = r\"\"\"\n",
    "<head>\n",
    "    <style>\n",
    "        h1 {\n",
    "            color: black;\n",
    "            font-family: verdana;\n",
    "            font-size: 150%;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<crowd-form>\n",
    "    <video id=\"this_vid\">\n",
    "        <source src=\"{{ task.input.audioPath | grant_read_access }}\"\n",
    "            type=\"audio/mp4\">\n",
    "        Your browser does not support the audio element.\n",
    "    </video>\n",
    "    <br />\n",
    "    <br />\n",
    "    <crowd-button onclick=\"onClick(); return false;\"><h1> Click to play video section!</h1></crowd-button>\n",
    "    <br />\n",
    "    Video title: <strong>{{ task.input.video_title }}</strong>\n",
    "    <br />\n",
    "\n",
    "    <h3>Instructions</h3>\n",
    "    <p>Transcribe the audio clip </p>\n",
    "    <p>The original transcript is <strong>\"{{ task.input.original_words }}\"</strong>.\n",
    "    If the text matches the audio, please retype the same transcription.</p>\n",
    "    <p>Ignore \"umms\", \"hmms\", \"uhs\" and other non-textual phrases. </p>\n",
    "    <p><strong>Important:</strong> If you encounter a technical term that has multiple words,\n",
    "    please <strong>hyphenate</strong> those words together. For example, \"k nearest neighbors\" should be transcribed as \"k-nearest-neighbors.\"</p>\n",
    "    <p>Click the space below to start typing.</p>\n",
    "    <crowd-text-area name=\"transcription\" rows=\"2\" label=\"Your transcription\" placeholder=\"Please enter the transcribed text.\"></crowd-text-area>\n",
    "\n",
    "    <full-instructions header=\"Transcription Instructions\">\n",
    "        <h2>Instructions</h2>\n",
    "        <p>Click the play button and listen carefully to the audio clip. Type what you hear in the box\n",
    "            below. Replay the clip by clicking the button again, as many times as needed.</p>\n",
    "    </full-instructions>\n",
    "\n",
    "</crowd-form>\n",
    "\n",
    "<script>\n",
    "    var video = document.getElementById('this_vid');\n",
    "    video.onloadedmetadata = function() {\n",
    "        video.currentTime = {{ task.input.start_time }};\n",
    "    };\n",
    "    function onClick() {\n",
    "        video.pause();\n",
    "        video.currentTime = {{ task.input.start_time }};\n",
    "        video.play();\n",
    "        video.ontimeupdate = function () {\n",
    "            if (video.currentTime >= {{ task.input.end_time }}) {\n",
    "                video.pause()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "#t={{ task.input.start_time }},{{ task.input.end_time }}\n",
    "\n",
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `{{ task.input.audioPath | grant_read_access }}` field allows you to grant access to and display a video using a path to the video’s location in an S3 bucket. To prevent the reviewer from navigating to irrelevant sections of the video, the `controls` parameter is omitted from the video tag and a single replay button is included to control which section can be replayed.\n",
    "\n",
    "Below the video player, the `<crowd-text-area>` HTML tag creates a submission form that your reviewer will use to type and submit.\n",
    "\n",
    "At the end of the HTML snippet, the `<script>` tag contains the logic for the replay button. The `{{ task.input.start_time }}` and `{{ task.input.end_time }}` fields allow you to inject the start and end times of the video subsection you want transcribed for the current task.\n",
    "\n",
    "Now let's create a Human Task UI resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-west-2:688520471316:human-task-ui/ui-transcribe-02e9e0fb-3bc1-4f98-b280-c0bd12a93e14\n"
     ]
    }
   ],
   "source": [
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = 'ui-transcribe-' + str(uuid.uuid4()) \n",
    "\n",
    "# Create task UI\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Definition\n",
    "\n",
    "In this section, we're going to create a flow definition definition. Flow Definitions allow us to specify:\n",
    "\n",
    "* The workforce that your tasks will be sent to.\n",
    "* The instructions that your workforce will receive. This is called a worker task template.\n",
    "* The configuration of your worker tasks, including the number of workers that receive a task and time limits to complete tasks.\n",
    "* Where your output data will be stored.\n",
    "\n",
    "This demo is going to use the API, but you can optionally create this workflow definition in the console as well.\n",
    "\n",
    "For more details and instructions, see [here](https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow definition name - this value is unique per account and region. You can also provide your own value here.\n",
    "flowDefinitionName = 'fd-transcribe-demo-' + str(uuid.uuid4()) \n",
    "\n",
    "create_workflow_definition_response = sagemaker.create_flow_definition(\n",
    "        FlowDefinitionName= flowDefinitionName,\n",
    "        RoleArn= ROLE,\n",
    "        HumanLoopConfig= {\n",
    "            \"WorkteamArn\": WORKTEAM_ARN,\n",
    "            \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "            \"TaskCount\": 1,\n",
    "            \"TaskDescription\": \"Identify the word(s) spoken in the provided audio clip\",\n",
    "            \"TaskTitle\": \"Determine Words/Phrases of Audio Clip\" + str(datetime.now())\n",
    "        },\n",
    "        OutputConfig={\n",
    "            \"S3OutputPath\" : OUTPUT_PATH\n",
    "        }\n",
    "    )\n",
    "flowDefinitionArn = create_workflow_definition_response['FlowDefinitionArn'] # let's save this ARN for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Active\n",
      "Flow Definition is active\n"
     ]
    }
   ],
   "source": [
    "# Describe flow definition - status should be active\n",
    "for x in range(60):\n",
    "    describeFlowDefinitionResponse = sagemaker.describe_flow_definition(FlowDefinitionName=flowDefinitionName)\n",
    "    print(describeFlowDefinitionResponse['FlowDefinitionStatus'])\n",
    "    if (describeFlowDefinitionResponse['FlowDefinitionStatus'] == 'Active'):\n",
    "        print(\"Flow Definition is active\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Loops\n",
    "### Sending sequences of words/phrases of low confidence for review\n",
    "After setting up our Flow Definition, we're ready to use Amazon Transcribe and initiate human loops. While iterating through the list of transcribed words and their confidence scores, we create a HumanLoop task whenever the confidence score is below some threshold, `CONFIDENCE_SCORE_THRESHOLD`.\n",
    "\n",
    "An important thing to consider is how do we deal with a low-confidence word that is part of a phrase that was also mis-transcribed? To handle these cases, let’s write a function that gets the sequence of words centered about a given index, and the sequence's starting and ending timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to get the words near a word with poor confidence,\n",
    "# since it is possible that the transcription also mis-transcribed nearby words/phrases\n",
    "def get_word_neighbors(words, index):\n",
    "    \"\"\"\n",
    "    gets the words transcribe found at most 3 away from the input index\n",
    "    Returns:\n",
    "        list: words at most 3 away from the input index\n",
    "        int: starting time of the first word in the list\n",
    "        int: ending time of the last word in the list\n",
    "    \"\"\"\n",
    "    i = max(0, index - 3)\n",
    "    j = min(len(words) - 1, index + 3)\n",
    "    return words[i: j + 1], words[i][\"start_time\"], words[j][\"end_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for every word we encounter with low confidence, we send its associated sequence of neighboring words for human review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable names for convenience\n",
    "# all_entire_transcript = []\n",
    "# all_sentences_and_times = []\n",
    "# all_confidences = []\n",
    "# all_scores = []\n",
    "# job_names = [\n",
    "#     \"AWS-sage-vid-1\",\n",
    "#     \"AWS-sage-vid-2\",\n",
    "#     \"AWS-sage-vid-3\",\n",
    "#     \"AWS-sage-vid-4\",\n",
    "#     \"AWS-sage-vid-5\",\n",
    "#     \"AWS-sage-vid-6\",\n",
    "#     \"AWS-sage-vid-7\",\n",
    "#     \"AWS-sage-vid-8\",\n",
    "#     \"AWS-sage-vid-9\",\n",
    "#     \"AWS-sage-vid-10\",\n",
    "# ]\n",
    "\n",
    "# # Audio file path\n",
    "# # job_uri_s3 = f\"s3://{BUCKET}/Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive.mp4\"\n",
    "\n",
    "# # Path to folder\n",
    "# folder_path = f\"s3://{BUCKET}/transcribe-bucket/\"\n",
    "\n",
    "# all_videos = [\n",
    "#              'Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive.mp4',\n",
    "#              'Built-in Machine Learning Algorithms with Amazon SageMaker - a Deep Dive.mp4',\n",
    "#              'Bring Your Own Custom ML Models with Amazon SageMaker.mp4',\n",
    "#              'Train Your ML Models Accurately with Amazon SageMaker.mp4',\n",
    "#              'Deploy Your ML Models to Production at Scale with Amazon SageMaker.mp4',\n",
    "#              'Tune Your ML Models to the Highest Accuracy with Amazon SageMaker Automatic Model Tuning.mp4',\n",
    "#              'Scale up Training of Your ML Models with Distributed Training on Amazon SageMaker.mp4',\n",
    "#              'Use the Deep Learning Framework of Your Choice with Amazon SageMaker.mp4',\n",
    "#              'Learn to Analyze the Co-Relation in Your Datasets Using Feature Engineering with Amazon SageMake.mp4',\n",
    "#              'Get Scheduled Predictions on Your ML Models with Amazon SageMaker Batch Transform.mp4'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_entire_transcript[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive.mp4 =========\n",
      "The original transcription is \"show up Under are easy to console \"\n",
      "The original transcription is \"And more cores see is compute optimized \"\n",
      "The original transcription is \"every version of Annecy two instance is \"\n",
      "The original transcription is \"distributing data sets wanted by putt mode \"\n",
      "The original transcription is \"onto your EBS volumes And again that's \"\n",
      "The original transcription is \"of those example No books are open \"\n",
      "The original transcription is \"the two main ones markdown is gonna \"\n",
      "The original transcription is \"I started using Boto three but I \"\n",
      "The original transcription is \"absolutely upgrade on bits fun because you \"\n",
      "The original transcription is \"That's the python Asi que We're getting \"\n",
      "The original transcription is \"the Internet s Oh this is from \"\n",
      "The original transcription is \"this is from Sarraf He's the author \"\n",
      "The original transcription is \"right up here then the title of \"\n",
      "The original transcription is \"but definitely use Lambda to turn your \"\n",
      "The original transcription is \"then edit your ec2 instance or the \"\n",
      "Number of tasks sent to review: 15\n"
     ]
    }
   ],
   "source": [
    "# Sample data, human loop started\n",
    "human_loops_started = []\n",
    "CONFIDENCE_SCORE_THRESHOLD = 0.4\n",
    "MARGIN = 3\n",
    "\n",
    "count = 0\n",
    "for index in range(num_videos):\n",
    "    this_uri = folder_path+all_videos[index]\n",
    "    this_confidences = all_confidences[index]\n",
    "    \n",
    "    print(\"========= \" + all_videos[index] + \" =========\")\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(this_confidences):\n",
    "        word = this_confidences[i][\"content\"]\n",
    "        neighbors, start_time, end_time = get_word_neighbors(this_confidences, i)\n",
    "\n",
    "        # Our condition for when we want to engage a human for review\n",
    "        if (this_confidences[i][\"confidence\"] < CONFIDENCE_SCORE_THRESHOLD):\n",
    "\n",
    "            # get the original sequence of words\n",
    "            sequence = \"\"\n",
    "            for block in neighbors:\n",
    "                sequence += block['content'] + \" \"\n",
    "\n",
    "            humanLoopName = str(uuid.uuid4())\n",
    "            # \"initialValue\": word,\n",
    "            inputContent = {\n",
    "                \"audioPath\": this_uri,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"original_words\": sequence,\n",
    "                \"video_title\": all_videos[index]\n",
    "            }\n",
    "            start_loop_response = a2i.start_human_loop(\n",
    "                HumanLoopName=humanLoopName,\n",
    "                FlowDefinitionArn=flowDefinitionArn,\n",
    "                HumanLoopInput={\n",
    "                    \"InputContent\": json.dumps(inputContent)\n",
    "                }\n",
    "            )\n",
    "            human_loops_started.append(humanLoopName)\n",
    "            # print(f'Confidence score of {obj[\"confidence\"]} is less than the threshold of {CONFIDENCE_SCORE_THRESHOLD}')\n",
    "            # print(f'Starting human loop with name: {humanLoopName}')\n",
    "            # print(f'Sending words from times {start_time} to {end_time} to review')\n",
    "            print(f'The original transcription is \"{sequence}\"')\n",
    "            \n",
    "            count = count + 1\n",
    "            \n",
    "            # Advance to next word after the margin away from the low-confidence word\n",
    "            i = i + MARGIN + 1\n",
    "        else:\n",
    "            # No human loop created, advance to next word.\n",
    "            i = i + 1\n",
    "            # print(f'SentimentScore of {obj[\"confidence\"]} is above threshold of {CONFIDENCE_SCORE_THRESHOLD}')\n",
    "            # print('No human loop created. \\n')\n",
    "        \n",
    "\n",
    "print(f'Number of tasks sent to review: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also save the name of each human loop, in case we need to retrieve them later after shutting down this notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab39bc61-5f27-4a2c-89df-ee807e44b42b\n",
      "fc50a8ea-5cbf-47ec-a592-b6f4273a9470\n",
      "8b1c619f-6d21-4748-89ce-f235624406da\n",
      "32170f66-7fd3-48dd-a372-2097e5bbd429\n",
      "377b46d6-2dc1-48bf-a70b-69ced736720b\n",
      "a00bdd3f-2922-4b15-9e02-9f6e88654d79\n",
      "e731646f-43dd-4b77-9854-ced871d8552b\n",
      "13a76aca-1af9-4325-ad95-784b6e09edbc\n",
      "ee852579-c2cc-40fb-a691-b7f0321a15da\n",
      "d89457b7-d43d-4eb3-b97d-3af315d17017\n",
      "fd1365e7-119b-4932-a947-cae08ef67d7c\n",
      "5ca418c4-c0f4-49e1-b4e1-128e0fb56ffe\n",
      "1c321e10-9157-4648-b8f5-77111605e8b1\n",
      "26d7fa25-91d0-4e18-86d4-8dcede85b433\n",
      "7fcb2c3b-be9c-429d-a177-fc5121a7c0ac\n"
     ]
    }
   ],
   "source": [
    "file_hl = open(\"human_loops_names.txt\",\"w\") \n",
    "for name in human_loops_started:\n",
    "    file_hl.write(name + \"\\n\") \n",
    "file_hl.close()\n",
    "!cat human_loops_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanLoop Name: 8680d6c4-8ef7-499c-b628-370c212c2589\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 68897b14-f3e5-4fbd-bb10-723307db201d\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: a30f6cf6-49e8-4042-ab52-5a185b1b84ab\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 3d11eca1-aa1d-4e67-82e4-88bb77c27788\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 4e3103b3-5255-496f-92d6-2cb3006cd402\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 8c015c14-aa9f-4ac4-b44e-b29f84ada6f7\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 02563f0f-5ca4-46ad-b2b4-c78efceba1dd\n",
      "HumanLoop Status: InProgress\n"
     ]
    }
   ],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    # print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    # print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait For Workers to Complete Task\n",
    "We display the link to the private worker portal here for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\n",
      "https://v3t960yxw8.labeling.us-west-2.sagemaker.aws\n"
     ]
    }
   ],
   "source": [
    "# Wait For Workers to Complete Task\n",
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\")\n",
    "print('https://' + sagemaker.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanLoop Name: ab39bc61-5f27-4a2c-89df-ee807e44b42b\n",
      "HumanLoop Status: Completed\n",
      "HumanLoop Name: fc50a8ea-5cbf-47ec-a592-b6f4273a9470\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 8b1c619f-6d21-4748-89ce-f235624406da\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 32170f66-7fd3-48dd-a372-2097e5bbd429\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 377b46d6-2dc1-48bf-a70b-69ced736720b\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: a00bdd3f-2922-4b15-9e02-9f6e88654d79\n",
      "HumanLoop Status: Completed\n",
      "HumanLoop Name: e731646f-43dd-4b77-9854-ced871d8552b\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 13a76aca-1af9-4325-ad95-784b6e09edbc\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: ee852579-c2cc-40fb-a691-b7f0321a15da\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: d89457b7-d43d-4eb3-b97d-3af315d17017\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: fd1365e7-119b-4932-a947-cae08ef67d7c\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 5ca418c4-c0f4-49e1-b4e1-128e0fb56ffe\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 1c321e10-9157-4648-b8f5-77111605e8b1\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 26d7fa25-91d0-4e18-86d4-8dcede85b433\n",
      "HumanLoop Status: InProgress\n",
      "HumanLoop Name: 7fcb2c3b-be9c-429d-a177-fc5121a7c0ac\n",
      "HumanLoop Status: InProgress\n"
     ]
    }
   ],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    # print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    # print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Task Results\n",
    "\n",
    "Once work is completed, Amazon A2I stores results in your S3 bucket and sends a Cloudwatch event. Your results should be available in the S3 `OUTPUT_PATH` when all work is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': 'asdfasdfasdf'}\n",
      "{'transcription': 'dsfgsdfgsdfg'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "    output_bucket_key = splitted_string[1]\n",
    "\n",
    "    response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    pp.pprint(json_output['humanAnswers'][0]['answerContent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Custom vocabularies using A2I results\n",
    "\n",
    "Using the corrected transcriptions from our human reviewers, let’s parse through these results to identify the domain-specific terms that we want to add to a custom vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve A2I results\n",
    "To get the technical terms identified by human review, we first accumulate all human-reviewed words into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "corrected_words = []\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "    output_bucket_key = splitted_string[1]\n",
    "\n",
    "    response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    \n",
    "    # add the human-reviewed answers split by spaces\n",
    "    corrected_words += [word.strip(punctuation).lower() for word in json_output['humanAnswers'][0]['answerContent']['transcription'].split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2\n"
     ]
    }
   ],
   "source": [
    "# print(\"...ec2...\".strip(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corrected_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out common English words\n",
    "Now, we want to parse through these words and look for “uncommon” English words. An easy way to do this is to use a large English corpus and verify whether each of our human-reviewed words exists in this corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary of English words\n",
    "# Note that this corpus of words is not 100% exhaustive\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "my_dict=set(words.words()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing contractions\n",
    "# https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "def remove_contractions(word_list):\n",
    "    return [word for word in word_list if word not in contractions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Technical/Uncommon Words\n",
    "After removing contractions, human-reviewed words that are not in the English language corpus are likely to be the technical terms we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set([])\n",
    "for word in remove_contractions(corrected_words):\n",
    "    if word:\n",
    "        if word.lower() not in my_dict:\n",
    "            if word.endswith('s') and word[:-1] in my_dict:\n",
    "                continue\n",
    "            elif word.endswith(\"'s\") and word[:-2] in my_dict:\n",
    "                continue\n",
    "            else:\n",
    "                word_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "including\n",
      "machine-learning\n",
      "grabbing\n",
      "amazon\n",
      "boto3\n",
      "started\n",
      "t3\n",
      "called\n",
      "sarab\n",
      "ecr\n",
      "using\n",
      "ebs\n",
      "internet\n",
      "jupyter\n",
      "distributing\n",
      "opt/ml\n",
      "optimized\n",
      "desktop\n",
      "tokenizing\n",
      "s3\n",
      "sdk\n",
      "encrypted\n",
      "relying\n",
      "sagemaker\n",
      "mars-dot-r\n",
      "datasets\n",
      "upload\n",
      "iam\n",
      "gonna\n",
      "managing\n",
      "wanna\n",
      "vpc\n",
      "managed\n",
      "mars.r\n",
      "ec2\n",
      "blazingtext\n"
     ]
    }
   ],
   "source": [
    "for word in word_set:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Vocabulary\n",
    "Using the technical terms identified above, we manually created a custom vocabulary of those terms that we want Transcribe to be able to recognize. A custom vocabulary table enables options to tell Amazon Transcribe how each technical term is pronounced and how it should be displayed.\n",
    "\n",
    "More details on how to form a custom vocabulary table can be found [here](https://docs.aws.amazon.com/transcribe/latest/dg/how-vocabulary.html#create-vocabulary-table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as you process additional videos on the same topic, you can keep updating this list, and the number of new technical terms you'll have to add will likely decrease each time you get a new video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_words=[['Phrase','IPA','SoundsLike','DisplayAs'], # This top line denote the column headers of the text file.\n",
    "                 ['machine-learning','','','machine learning'],\n",
    "                 ['amazon','','am-uh-zon','Amazon'],\n",
    "                 ['boto-three','','boe-toe-three','Boto3'],\n",
    "                 ['T.-three','','tee-three','T3'],\n",
    "                 ['Sarab','','suh-rob','Sarab'],\n",
    "                 ['E.C.R.','','ee-see-are','ECR'],\n",
    "                 ['E.B.S.','','ee-bee-ess','EBS'],\n",
    "                 ['jupyter','','joo-pih-ter','Jupyter'],\n",
    "                 ['opt-M.L.','','opt-em-ell','/opt/ml'],\n",
    "                 ['desktop','','desk-top','desktop'],\n",
    "                 ['S.-Three','','ess-three','S3'],\n",
    "                 ['S.D.K.','','ess-dee-kay','SDK'],\n",
    "                 ['sagemaker','','sage-may-ker','SageMaker'],\n",
    "                 ['mars-dot-r','','mars-dot-are','mars.R'],\n",
    "                 ['I.A.M.','','eye-ay-em','IAM'],\n",
    "                 ['V.P.C.','','','VPC'],\n",
    "                 ['E.C.-Two','','ee-see-too','EC2'],\n",
    "                 ['blazing-text','','','BlazingText'],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalized_words=[['Phrase','IPA','SoundsLike','DisplayAs'], # This top line denote the column headers of the text file.\n",
    "#                  ['E.C.-Two','','ee-see-too','EC2'],\n",
    "#                  ['E.C.-Two-instance','','ee-see-too-in-stunce','EC2 instance'],\n",
    "#                  ['lambda','','lam-duh','Lambda'],\n",
    "#                  ['S.D.K.','','ess-dee-kay','SDK'],\n",
    "#                  ['boto-three','','boe-toe-three','Boto3'],\n",
    "#                  ['S.-Three','','ess-three','S3'],\n",
    "#                  ['github','','git-hub','Github'],\n",
    "#                  ['sagemaker','','sage-may-ker','SageMaker'],\n",
    "#                  ['E.B.S.','','ee-bee-ess','EBS'],\n",
    "#                  ['G.P.U.','','gee-pee-you','GPU'],\n",
    "#                  ['git-repository','','git-ree-paw-zih-tor-ee','Git repository'],\n",
    "#                  ['jupyter','','joo-pih-ter','Jupyter'],\n",
    "#                  ['kernel','','ker-null','kernel'],\n",
    "#                  ['config','','con-fig','config'],\n",
    "#                  ['configs','','con-figs','configs'],\n",
    "#                  ['D.B.-pedia','','dee-bee-pee-dee-yuh','dbpedia'],\n",
    "#                  ['git-clone','','','git clone'],\n",
    "#                  ['notebook-instance','','','notebook instance'],\n",
    "#                  ['V.P.C.','','','VPC'],\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Table to a Txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vocab_file_name = \"customvocab4.txt\"\n",
    "file1 = open(custom_vocab_file_name,\"w\")\n",
    "template = '{}\\t{}\\t{}\\t{}\\n'\n",
    "for line in finalized_words:\n",
    "    file1.write(template.format(line[0],\n",
    "                                line[1],\n",
    "                                line[2],\n",
    "                                line[3])\n",
    "               )\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Custom Vocabulary File to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_file(custom_vocab_file_name, BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Custom Vocabulary\n",
    "After saving your custom vocabulary table to a text file and uploading it to an S3 bucket, create your custom vocabulary with a specified name so that Amazon Transcribe can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_improved='aws-sagemaker-vocab-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe = boto3.client(\"transcribe\")\n",
    "response = transcribe.create_vocabulary(\n",
    "    VocabularyName=vocab_improved,\n",
    "    LanguageCode='en-US',\n",
    "    VocabularyFileUri='s3://' + BUCKET + '/' + custom_vocab_file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'LanguageCode': 'en-US',\n",
      "    'ResponseMetadata': {   'HTTPHeaders': {   'connection': 'keep-alive',\n",
      "                                               'content-length': '93',\n",
      "                                               'content-type': 'application/x-amz-json-1.1',\n",
      "                                               'date': 'Wed, 17 Jun 2020 '\n",
      "                                                       '18:24:46 GMT',\n",
      "                                               'x-amzn-requestid': '31b73642-fdfb-49b6-b1cb-126d2db9acc9'},\n",
      "                            'HTTPStatusCode': 200,\n",
      "                            'RequestId': '31b73642-fdfb-49b6-b1cb-126d2db9acc9',\n",
      "                            'RetryAttempts': 0},\n",
      "    'VocabularyName': 'aws-sagemaker-vocab-5',\n",
      "    'VocabularyState': 'PENDING'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY\n"
     ]
    }
   ],
   "source": [
    "# Get the status of the vocab you created again (must wait until its VocabularyState is READY)\n",
    "response2 = transcribe.get_vocabulary(\n",
    "    VocabularyName=vocab_improved\n",
    ")\n",
    "print(response2['VocabularyState'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Improved Transcription using your Custom Vocabulary\n",
    "\n",
    "### Re-transcribe using the Custom Vocabulary\n",
    "Let's re-transcribe our two test videos using our custom vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New job names\n",
    "# In-sample videos\n",
    "job_name_custom_vid_0='AWS-custom-0-using-' + vocab_improved\n",
    "job_name_custom_vid_2='AWS-custom-2-using-' + vocab_improved\n",
    "\n",
    "# Out-sample videos\n",
    "job_name_custom_vid_1='AWS-custom-1-using-' + vocab_improved\n",
    "job_name_custom_vid_3='AWS-custom-3-using-' + vocab_improved\n",
    "\n",
    "job_names_custom = [job_name_custom_vid_0, job_name_custom_vid_1,\n",
    "                    job_name_custom_vid_2, job_name_custom_vid_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------mp4\n",
      "{'TranscriptionJob': {'TranscriptionJobName': 'AWS-custom-0-using-aws-sagemaker-vocab-5', 'TranscriptionJobStatus': 'IN_PROGRESS', 'LanguageCode': 'en-US', 'MediaSampleRateHertz': 44100, 'MediaFormat': 'mp4', 'Media': {'MediaFileUri': 's3://jashuang-sagemaker-5-22/transcribe-bucket/Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive.mp4'}, 'Transcript': {}, 'StartTime': datetime.datetime(2020, 6, 17, 20, 27, 7, 120000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 17, 20, 27, 7, 83000, tzinfo=tzlocal()), 'Settings': {'VocabularyName': 'aws-sagemaker-vocab-5', 'ChannelIdentification': False, 'ShowAlternatives': False}}, 'ResponseMetadata': {'RequestId': '4164820b-de4b-4dbc-94a4-6fc3d2ec3238', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Wed, 17 Jun 2020 20:27:09 GMT', 'x-amzn-requestid': '4164820b-de4b-4dbc-94a4-6fc3d2ec3238', 'x-amzn-transcribe-store-audio': 'false', 'content-length': '531', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "------mp4\n",
      "{'TranscriptionJob': {'TranscriptionJobName': 'AWS-custom-2-using-aws-sagemaker-vocab-5', 'TranscriptionJobStatus': 'IN_PROGRESS', 'LanguageCode': 'en-US', 'MediaSampleRateHertz': 44100, 'MediaFormat': 'mp4', 'Media': {'MediaFileUri': 's3://jashuang-sagemaker-5-22/transcribe-bucket/Bring Your Own Custom ML Models with Amazon SageMaker.mp4'}, 'Transcript': {}, 'StartTime': datetime.datetime(2020, 6, 17, 20, 27, 9, 497000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 17, 20, 27, 9, 475000, tzinfo=tzlocal()), 'Settings': {'VocabularyName': 'aws-sagemaker-vocab-5', 'ChannelIdentification': False, 'ShowAlternatives': False}}, 'ResponseMetadata': {'RequestId': '84d1e827-f7fe-4eff-bb6d-dbb68dd6c906', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Wed, 17 Jun 2020 20:27:11 GMT', 'x-amzn-requestid': '84d1e827-f7fe-4eff-bb6d-dbb68dd6c906', 'x-amzn-transcribe-store-audio': 'false', 'content-length': '517', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Start another transcription job using your custom vocabulary.\n",
    "transcribe(job_name_custom_vid_0, folder_path+all_videos[0], BUCKET, vocab_name=vocab_improved)\n",
    "# Start another transcription job using your custom vocabulary.\n",
    "transcribe(job_name_custom_vid_2, folder_path+all_videos[2], BUCKET, vocab_name=vocab_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------mp4\n",
      "{'TranscriptionJob': {'TranscriptionJobName': 'AWS-custom-3-using-aws-sagemaker-vocab-5', 'TranscriptionJobStatus': 'IN_PROGRESS', 'LanguageCode': 'en-US', 'MediaSampleRateHertz': 44100, 'MediaFormat': 'mp4', 'Media': {'MediaFileUri': 's3://jashuang-sagemaker-5-22/transcribe-bucket/Train Your ML Models Accurately with Amazon SageMaker.mp4'}, 'Transcript': {}, 'StartTime': datetime.datetime(2020, 6, 17, 18, 34, 6, 41000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 17, 18, 34, 6, 14000, tzinfo=tzlocal()), 'Settings': {'VocabularyName': 'aws-sagemaker-vocab-5', 'ChannelIdentification': False, 'ShowAlternatives': False}}, 'ResponseMetadata': {'RequestId': '171135b5-0e2a-4b96-99de-91d11765eed7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Wed, 17 Jun 2020 18:34:08 GMT', 'x-amzn-requestid': '171135b5-0e2a-4b96-99de-91d11765eed7', 'x-amzn-transcribe-store-audio': 'false', 'content-length': '517', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Start another transcription job using your custom vocabulary.\n",
    "transcribe(job_name_custom_vid_1, folder_path+all_videos[1], BUCKET, vocab_name=vocab_improved)\n",
    "# Start another transcription job using your custom vocabulary.\n",
    "transcribe(job_name_custom_vid_3, folder_path+all_videos[3], BUCKET, vocab_name=vocab_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert to for loop\n",
    "\n",
    "all_entire_transcript_custom = []\n",
    "all_sentences_and_times_custom = []\n",
    "all_confidences_custom = []\n",
    "all_scores_custom = []\n",
    "for i in range(0,4):\n",
    "    entire_transcript_1, sentences_and_times_1, confidences_1, scores_1 = get_transcript_text_and_timestamps(BUCKET,job_names_custom[i]+\".json\")\n",
    "    all_entire_transcript_custom.append(entire_transcript_1)\n",
    "    all_sentences_and_times_custom.append(sentences_and_times_1)\n",
    "    all_confidences_custom.append(confidences_1)\n",
    "    all_scores_custom.append(scores_1)\n",
    "\n",
    "# # In-sample\n",
    "# entire_transcript_0_custom,sentences_and_times_0_custom, confidences_0_custom, scores_0_custom = get_transcript_text_and_timestamps(BUCKET,\n",
    "#                                                                                                       job_name_custom_vid_0+\".json\")\n",
    "# entire_transcript_2_custom,sentences_and_times_2_custom, confidences_2_custom, scores_2_custom = get_transcript_text_and_timestamps(BUCKET,\n",
    "#                                                                                                       job_name_custom_vid_2+\".json\")\n",
    "\n",
    "# # Out-sample\n",
    "# entire_transcript_1_custom,sentences_and_times_1_custom, confidences_1_custom, scores_1_custom = get_transcript_text_and_timestamps(BUCKET,\n",
    "#                                                                                                       job_name_custom_vid_1+\".json\")\n",
    "# entire_transcript_3_custom,sentences_and_times_3_custom, confidences_3_custom, scores_3_custom = get_transcript_text_and_timestamps(BUCKET,\n",
    "#                                                                                                       job_name_custom_vid_3+\".json\")\n",
    "                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_sentences_and_times_custom[3][-3])\n",
    "# print(sentences_and_times_3_custom[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Improved Transcripts to Txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the improved transcripts\n",
    "i = 1\n",
    "for list_ in all_sentences_and_times_custom:   \n",
    "    file = open(f\"improved_transcript_{i}.txt\",\"w\")\n",
    "    for tup in list_:\n",
    "        file.write(tup['sentence'] + \"\\n\") \n",
    "    file.close()\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we have two transcript versions per video — one using default parameters and one using our custom vocabulary. To compare these two transcripts, the last thing we need is a “ground truth” transcript, i.e., an answer key. For this demo, we’ve created ground truth transcripts for each of the four SageMaker videos, which you can find in the Github repository.\n",
    "\n",
    "### Calculating Word Error Rate (WER)\n",
    "The most common metric for speech recognition accuracy is called word error rate (WER), which can be roughly defined to be the proportion of transcription errors relative to the number of words that were actually said. More details can be found here (https://en.wikipedia.org/wiki/Word_error_rate).\n",
    "\n",
    "We'll be using a lightweight open-source Python library called JiWER for calculating WER between transcripts.\n",
    "\n",
    "For more details, see the open-source [description](https://pypi.org/project/jiwer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example\n",
    "ground_truth = \"hello world\"\n",
    "hypothesis = \"hello duck\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(ground_truth, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformation function to preprocess transcript\n",
    "transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.SentencesToListOfWords(),\n",
    "    jiwer.SentencesToListOfWords(word_delimiter=\" \"),\n",
    "    jiwer.RemoveEmptyStrings()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample video metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== In-sample videos ==========\n",
      "Processing video #1\n",
      "The baseline accuracy (before using custom vocabularies) is 0.05184.\n",
      "The new accuracy (after using custom vocabularies) is 0.02624.\n",
      "Processing video #3\n",
      "The baseline accuracy (before using custom vocabularies) is 0.11940298507462686.\n",
      "The new accuracy (after using custom vocabularies) is 0.07835820895522388.\n"
     ]
    }
   ],
   "source": [
    "print(\"========== In-sample videos ==========\")\n",
    "for index in [1,3]:\n",
    "    print(f\"Processing video #{index}\")\n",
    "    # Original transcript\n",
    "    hypothesis_original = \"\"\n",
    "    f1 = open(f\"original_transcript_{index}.txt\", \"r\")\n",
    "    for line in f1:\n",
    "        hypothesis_original += (line.strip() + \" \")\n",
    "    f1.close()\n",
    "    \n",
    "    # Transcript after custom vocabulary\n",
    "    hypothesis_2 = \"\"\n",
    "    f2 = open(f\"improved_transcript_{index}.txt\", \"r\")\n",
    "    for line in f2:\n",
    "        hypothesis_2 += (line.strip() + \" \")\n",
    "    f2.close()\n",
    "    \n",
    "    # Ground truth transcript\n",
    "    ground_truth = \"\"\n",
    "    f3 = open(f\"ground_truth_{index}.txt\", \"r\")\n",
    "    for line in f3:\n",
    "        ground_truth += (line.strip() + \" \")\n",
    "    f3.close()\n",
    "    \n",
    "    # Calculate baseline accuracy\n",
    "    baseline_accuracy = jiwer.wer(\n",
    "        ground_truth, \n",
    "        hypothesis_original, \n",
    "        truth_transform=transformation, \n",
    "        hypothesis_transform=transformation\n",
    "    )\n",
    "    \n",
    "    print(f\"The baseline accuracy (before using custom vocabularies) is {baseline_accuracy}.\")\n",
    "    \n",
    "    # Calculate new accuracy after custom vocabulary\n",
    "    new_accuracy = jiwer.wer(\n",
    "        ground_truth,\n",
    "        hypothesis_2, \n",
    "        truth_transform=transformation, \n",
    "        hypothesis_transform=transformation\n",
    "    )\n",
    "    \n",
    "    print(f\"The new accuracy (after using custom vocabularies) is {new_accuracy}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-sample video metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Out-sample videos ==========\n",
      "Processing video #2\n",
      "The baseline accuracy (beforegot using custom vocabularies) is 0.07547814207650273.\n",
      "The new accuracy (after using custom vocabularies) is 0.06557377049180328.\n",
      "Processing video #4\n",
      "The baseline accuracy (beforegot using custom vocabularies) is 0.10906969962088073.\n",
      "The new accuracy (after using custom vocabularies) is 0.08982210557013706.\n"
     ]
    }
   ],
   "source": [
    "print(\"========== Out-sample videos ==========\")\n",
    "for index in [2,4]:\n",
    "    print(f\"Processing video #{index}\")\n",
    "    # Original transcript\n",
    "    hypothesis_original = \"\"\n",
    "    f1 = open(f\"original_transcript_{index}.txt\", \"r\")\n",
    "    for line in f1:\n",
    "        hypothesis_original += (line.strip() + \" \")\n",
    "    f1.close()\n",
    "    \n",
    "    # Transcript after custom vocabulary\n",
    "    hypothesis_2 = \"\"\n",
    "    f2 = open(f\"improved_transcript_{index}.txt\", \"r\")\n",
    "    for line in f2:\n",
    "        hypothesis_2 += (line.strip() + \" \")\n",
    "    f2.close()\n",
    "    \n",
    "    ground_truth = \"\"\n",
    "    f3 = open(f\"ground_truth_{index}.txt\", \"r\")\n",
    "    for line in f3:\n",
    "        ground_truth += (line.strip() + \" \")\n",
    "    f3.close()\n",
    "    \n",
    "    # Calculate baseline accuracy\n",
    "    baseline_accuracy = jiwer.wer(\n",
    "        ground_truth, \n",
    "        hypothesis_original, \n",
    "        truth_transform=transformation, \n",
    "        hypothesis_transform=transformation\n",
    "    )\n",
    "    \n",
    "    print(f\"The baseline accuracy (beforegot using custom vocabularies) is {baseline_accuracy}.\")\n",
    "    \n",
    "    # Calculate new accuracy after custom vocabulary\n",
    "    new_accuracy = jiwer.wer(\n",
    "        ground_truth,\n",
    "        hypothesis_2, \n",
    "        truth_transform=transformation, \n",
    "        hypothesis_transform=transformation\n",
    "    )\n",
    "    \n",
    "    print(f\"The new accuracy (after using custom vocabularies) is {new_accuracy}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the new transcript (after applying the custom vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis_2 = \"\"\n",
    "# f2 = open(\"improvedtranscript_2.txt\", \"r\")\n",
    "# for line in f2:\n",
    "#     if line.strip() == \"--STOP--\":\n",
    "#         break\n",
    "#     hypothesis_2 += (line.strip() + \" \")\n",
    "# f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the \"Ground Truth\" transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth_2 = \"\"\n",
    "# f1 = open(\"ground_truth.txt\", \"r\")\n",
    "# for line in f1:\n",
    "#     if line.strip() == \"--STOP--\":\n",
    "#         break\n",
    "#     ground_truth_2 += (line.strip() + \" \")\n",
    "# f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jiwer.wer(\n",
    "#     ground_truth_2, \n",
    "#     hypothesis_2_original, \n",
    "#     truth_transform=transformation, \n",
    "#     hypothesis_transform=transformation\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute New Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jiwer.wer(\n",
    "#     ground_truth_2, \n",
    "#     hypothesis_2, \n",
    "#     truth_transform=transformation, \n",
    "#     hypothesis_transform=transformation\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "We've provided a table summarizing the changes in WER scores below. Note that it's possible you may get slightly different numbers by random chance.\n",
    "\n",
    "|            | Video | Baseline WER (before custom vocabulary) | New WER (after custom vocabulary) | Percentage Change |\n",
    "|------------|-------|-----------------------------------------|------------------------------------|-------------------|\n",
    "| In-sample  | #1     | 5.18%                                   | 2.62%                              | -49%              |\n",
    "| In-sample  | #3     | 11.94%                                  | 7.84%                              | -34%              |\n",
    "| Out-sample | #2     | 7.55%                                   | 6.56%                              | -13%              |\n",
    "| Out-sample | #4     | 10.91%                                  | 8.98%                              | -18%              |\n",
    "\n",
    "There are several ways to interpret these results.\n",
    "\n",
    "Let's first consider the percentage change column. By definition, we have $WER=\\frac{S+D+I}{N}$, where $S$, $D$, and $I$ are the number of substitution, deletion, and insertion operations, respectively, needed to get from the outputted transcript to the ground truth, and $N$ is the total number of words. The percentage change is then the percentage change in the number of operations needed, so the decreases that we observed look pretty good.\n",
    "\n",
    "If we consider absolute WER scores, the initial WER of 5.18%, for instance, might already feel sufficiently low — that's only around 1 in 20 words that are mis-transcribed! However, this rate can be misleading, since domain-specific terms are often the least common words spoken (relative to frequent words like “to,” “and,” “I” etc.) but the most commonly mis-transcribed. For applications like search engine optimization (SEO) and video organization by topic, it could be critical that these technical terms are transcribed correctly. Let’s take a look at how our custom vocabulary impacted the transcription of several important technical terms:\n",
    "\n",
    "#### In-sample videos:\n",
    "\n",
    "Video #1:\n",
    "\n",
    "| Technical Term | Ground Truth mentions | Original Transcript Mentions | New Transcript Mentions | Percentage Point Change |\n",
    "|----------------|-----------------------|------------------------------|-------------------------|-------------------------|\n",
    "| SageMaker      | 22                    | 4 (18%)                      | 22 (100%)               | +82%                    |\n",
    "| EC2            | 15                    | 1 (7%)                       | 15 (100%)               | +93%                    |\n",
    "| EBS            | 11                    | 7 (64%)                      | 11 (100%)                | +36%                    |\n",
    "| Jupyter        | 5                     | 0 (0%)                       | 5 (100%)                | +100%                   |\n",
    "| S3             | 3                     | 0 (0%)                       | 3 (100%)                | +100%                   |\n",
    "| SDK            | 2                     | 0 (0%)                        | 2 (100%)                | +100%                 |\n",
    "| BlazingText    | 2                     | 0 (0%)                        | 2 (100%)                | +100%                 |\n",
    "| IAM            | 1                     | 0 (0%)                        | 1 (100%)                | +100%                 |\n",
    "| **Total**      | **61**               | **12 (20%)**                  | **61 (100%)**           | **+80%**               |\n",
    "\n",
    "Video #3:\n",
    "\n",
    "| Technical Term | Ground Truth mentions | Original Transcript Mentions | New Transcript Mentions | Percentage Point Change |\n",
    "|----------------|-----------------------|------------------------------|-------------------------|-------------------------|\n",
    "| SageMaker      | 17                    | 4 (24%)                      | 17 (100%)               | +76%                    |\n",
    "| ECR            | 7                     | 0 (0%)                       | 7 (100%)                | +100%                    |\n",
    "| /opt/ml        | 6                     | 0 (0%)                       | 6 (100%)                | +100%                    |\n",
    "| mars.R         | 3                     | 0 (0%)                       | 3 (100%)                | +100%                   |\n",
    "| S3             | 1                     | 0 (0%)                       | 1 (100%)                | +100%                   |\n",
    "| **Total**      | **34**               | **4 (12%)**                  | **34 (100%)**           | **+88%**               |\n",
    "\n",
    "\n",
    "#### Out-sample videos:\n",
    "\n",
    "Video #2:\n",
    "\n",
    "| Technical Term | Ground Truth mentions | Original Transcript mentions | New Transcript mentions | Percentage Point Change |\n",
    "|----------------|-----------------------|------------------------------|-------------------------|-------------------------|\n",
    "| SageMaker      | 12                    | 3 (25%)                      | 12 (100%)               | +75%                    |\n",
    "| BlazingText    | 3                     | 0 (0%)                       | 3 (100%)                | +100%                   |\n",
    "| ECR            | 1                     | 0 (0%)                       | 1 (100%)                | +100%                   |\n",
    "| **Total**      | **16**               | **3 (19%)**                  | **16 (100%)**           | **+81%**               |\n",
    "\n",
    "Video #4:\n",
    "\n",
    "| Technical Term | Ground Truth mentions | Original Transcript mentions | New Transcript mentions | Percentage Point Change |\n",
    "|----------------|-----------------------|------------------------------|-------------------------|-------------------------|\n",
    "| SageMaker      | 21                    | 4 (19%)                      | 20 (95%)                | +75%                    |\n",
    "| EC2            | 11                    | 0 (0%)                       | 11 (100%)               | +100%                    |\n",
    "| S3             | 7                     | 0 (0%)                       | 6 (86%)                 | +86%                    |\n",
    "| ECR            | 2                     | 0 (0%)                       | 2 (100%)                | +100%                   |\n",
    "| SDK            | 1                     | 0 (0%)                       | 1 (100%)                | +100%                   |\n",
    "| EBS            | 1                     | 1 (100%)                     | 1 (100%)                | +0%                     |\n",
    "| **Total**      | **43**               | **3 (12%)**                  | **41 (95%)**           | **+83%**               |\n",
    "\n",
    "\n",
    "Now it does look like custom vocabularies were certainly worth the effort!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "To avoid incurring unnecessary charges, delete resources when not in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In this post, we walked through an example of how you can improve transcripts from Amazon Transcribe using custom vocabularies and an Amazon A2I human review workflow. This allows you to quickly identify domain-specific terms using your own private workforce and review workflows, and use these terms to build a custom vocabulary so that future mentions of term are transcribed with greater accuracy, at scale. Transcribing key technical terms correctly can be important for doing SEO, enabling highly specific textual queries, and grouping large quantities of video or audio files by technical terms.\n",
    "\n",
    "The full proof-of-concept Jupyter notebook can be found at this Github repository. Check out other blog posts covering integrations of Amazon A2I, such as [Using Amazon Textract with Amazon Augmented AI for processing critical documents](https://aws.amazon.com/blogs/machine-learning/using-amazon-textract-with-amazon-augmented-ai-for-processing-critical-documents/) and [Designing human review workflows with Amazon Translate and Amazon Augmented AI](https://aws.amazon.com/blogs/machine-learning/designing-human-review-workflows-with-amazon-translate-and-amazon-augmented-ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End!\n",
    "For a more detailed discussion with visuals, check out the accompanying blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
