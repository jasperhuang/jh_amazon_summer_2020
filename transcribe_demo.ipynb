{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Amazon Transcribe transcriptions using Custom Vocabularies, Amazon Augmented AI (A2I), and Amazon SageMaker.\n",
    "\n",
    "\n",
    "\n",
    "Visit https://github.com/aws-samples/amazon-a2i-sample-jupyter-notebooks for all A2I Sample Notebooks\n",
    "\n",
    "## Introduction\n",
    "blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Latest SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the latest installations of our dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install boto3 --upgrade\n",
    "!pip install -U botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region, Bucket, and Paths\n",
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles and Permissions\n",
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Setting Role to the default SageMaker Execution Role\n",
    "ROLE = get_execution_role()\n",
    "display(ROLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Transcription Job\n",
    "Here, we use Amazon Transcribe with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_1 = \"AWS-sage-1\"\n",
    "# audio file path\n",
    "job_uri_s3 = \"s3://jashuang-sagemaker-5-22/transcribe-bucket/Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive.mp4\"\n",
    "BUCKET = \"jashuang-sagemaker-5-22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a transcribe function\n",
    "def transcribe(job_name, job_uri, out_bucket, format=\"mp4\", vocab_name=None):\n",
    "    \"\"\"Transcribe a .wav or .mp4 file to text.\n",
    "    Args:\n",
    "        job_name (str): the name of the job that you specify;\n",
    "                        the output json will be job_name.json\n",
    "        job_uri (str): input path (in s3) to the file being transcribed\n",
    "        out_bucket (str): s3 bucket name that you want the output json\n",
    "                          to be placed in\n",
    "        format (str): mp4 or wav for input file format;\n",
    "                      defaults to mp4\n",
    "        vocab_name (str): name of custom vocabulary used;\n",
    "                          optional, defaults to None\n",
    "    \"\"\"\n",
    "    \n",
    "    if format not in ['mp3','mp4','wav','flac']:\n",
    "        print(\"Invalid format\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        transcribe = boto3.client(\"transcribe\")\n",
    "        print(\"------\" + format)\n",
    "        transcribe.start_transcription_job(\n",
    "            TranscriptionJobName=job_name,\n",
    "            Media={\"MediaFileUri\": job_uri},\n",
    "            MediaFormat=format,\n",
    "            LanguageCode=\"en-US\",\n",
    "            OutputBucketName=out_bucket,\n",
    "            Settings={'VocabularyName': vocab_name}\n",
    "        )\n",
    "        \n",
    "        while True:\n",
    "            status = transcribe.get_transcription_job(TranscriptionJobName=job_name)\n",
    "            if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "                break\n",
    "            print(\"Not ready yet...\")\n",
    "            time.sleep(5)\n",
    "        print(status)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe(job_name_1, job_uri_s3, BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Transcripts, Confidence Scores, and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript_text_and_timestamps(bucket_name, file_name):\n",
    "    \"\"\"take json file from s3 bucket and returns a tuple of:\n",
    "       entire transcript, list object of tuples of timestamp and individual sentences\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): name of s3 bucket\n",
    "        file_name (str): name of file\n",
    "    Returns:\n",
    "        (\n",
    "        entire_transcript: str,\n",
    "        sentences_and_times: [ {start_time (sec) : float,\n",
    "                                end_time (sec)   : float,\n",
    "                                sentence         : str,\n",
    "                                min_confidence   : float (minimum confidence score of that sentence)\n",
    "                                } ],\n",
    "        confidences:  [ {start_time (sec) : float,\n",
    "                         end_time (sec)   : float,\n",
    "                         content          : str, (single word/phrase)\n",
    "                         confidence       : float (confidence score of the word/phrase)\n",
    "                         } ],\n",
    "        scores: list of confidence scores\n",
    "        )\n",
    "    \"\"\"\n",
    "    s3_clientobj = s3.get_object(Bucket=bucket_name, Key=file_name)\n",
    "    s3_clientdata = s3_clientobj[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "    original = json.loads(s3_clientdata)\n",
    "    items = original[\"results\"][\"items\"]\n",
    "    entire_transcript = original[\"results\"][\"transcripts\"]\n",
    "\n",
    "    sentences_and_times = []\n",
    "    temp_sentence = \"\"\n",
    "    temp_start_time = 0\n",
    "    temp_min_confidence = 1.0\n",
    "    newSentence = True\n",
    "    \n",
    "    confidences = []\n",
    "    scores = []\n",
    "\n",
    "    i = 0\n",
    "    for item in items:\n",
    "        # always add the word\n",
    "        if item[\"type\"] == \"punctuation\":\n",
    "            temp_sentence = (\n",
    "                temp_sentence.strip() + item[\"alternatives\"][0][\"content\"] + \" \"\n",
    "            )\n",
    "        else:\n",
    "            temp_sentence = temp_sentence + item[\"alternatives\"][0][\"content\"] + \" \"\n",
    "            temp_min_confidence = min(temp_min_confidence,\n",
    "                                      float(item[\"alternatives\"][0][\"confidence\"]))\n",
    "            confidences.append({\"start_time\": float(item[\"start_time\"]),\n",
    "                                \"end_time\": float(item[\"end_time\"]),\n",
    "                                \"content\": item[\"alternatives\"][0][\"content\"],\n",
    "                                \"confidence\": float(item[\"alternatives\"][0][\"confidence\"])\n",
    "                               })\n",
    "            scores.append(float(item[\"alternatives\"][0][\"confidence\"]))\n",
    "\n",
    "        # if this is a new sentence, and it starts with a word, save the time\n",
    "        if newSentence == True:\n",
    "            if item[\"type\"] == \"pronunciation\":\n",
    "                temp_start_time = float(item[\"start_time\"])\n",
    "            newSentence = False\n",
    "        # else, keep going until you hit a punctuation\n",
    "        else:\n",
    "            if (\n",
    "                item[\"type\"] == \"punctuation\"\n",
    "                and item[\"alternatives\"][0][\"content\"] != \",\"\n",
    "            ):\n",
    "                # end time of sentence is end_time of previous word\n",
    "                end_time = items[i-1][\"end_time\"] if i-1 >= 0 else items[0][\"end_time\"]\n",
    "                sentences_and_times.append(\n",
    "                    {\"start_time\": temp_start_time,\n",
    "                     \"end_time\": end_time,\n",
    "                     \"sentence\": temp_sentence.strip(),\n",
    "                     \"min_confidence\": temp_min_confidence\n",
    "                    }\n",
    "                )\n",
    "                # reset the temp sentence and relevant variables\n",
    "                newSentence = True\n",
    "                temp_sentence = \"\"\n",
    "                temp_min_confidence = 1.0\n",
    "                \n",
    "        i = i + 1\n",
    "\n",
    "    return entire_transcript, sentences_and_times, confidences, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_transcript_1, sentences_and_times_1, confidences_1, scores_1 = get_transcript_text_and_timestamps(\"jashuang-sagemaker-5-22\",\"AWS-sage-1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_and_times_1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the first transcript to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file0 = open(\"originaltranscript.txt\",\"w\") \n",
    "for tup in sentences_and_times_1:\n",
    "    file0.write(tup['sentence'] + \"\\n\") \n",
    "file0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.xlim([min(scores)-0.1, max(scores)+0.1])\n",
    "plt.hist(scores, bins=20, alpha=0.5)\n",
    "plt.title('Plot of confidence scores')\n",
    "plt.xlabel('Confidence score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of low confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_scores = [i for i in scores if i < 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim([min(bad_scores)-0.1, 1.0])\n",
    "plt.hist(bad_scores, bins=20, alpha=0.5)\n",
    "plt.title('Plot of confidence scores less than 0.9')\n",
    "plt.xlabel('Confidence score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workteam or Workforce\n",
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKTEAM_ARN= \"arn:aws:sagemaker:us-west-2:688520471316:workteam/private-crowd/jashuang-test-workforce\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Amazon SageMaker client\n",
    "sagemaker = boto3.client('sagemaker', REGION)\n",
    "\n",
    "# Amazon Augment AI (A2I) client\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "\n",
    "s3 = boto3.client('s3', REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Control Plane Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Human Task UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<crowd-form>\n",
    "    <video controls>\n",
    "        <source src=\"{{ task.input.audioPath | grant_read_access }}#t={{ task.input.start_time }},{{ task.input.end_time }}\"\n",
    "            type=\"audio/mp4\">\n",
    "        Your browser does not support the audio element.\n",
    "    </video>\n",
    "    <p>If you need to replay the audio, please refresh the page.</p>\n",
    "\n",
    "    <h3>Instructions</h3>\n",
    "    <p>Transcribe the audio </p>\n",
    "    <p>Ignore \"umms\", \"hmms\", \"uhs\" and other non-textual phrases. </p>\n",
    "    <p>The original transcript is <strong>\"{{ task.input.original_words }}\"</strong>. If the text matches the audio, please retype the same transcription.</p>\n",
    "    <p>Click the space below to start typing.</p>\n",
    "    <crowd-text-area name=\"transcription\" rows=\"2\"></crowd-text-area>\n",
    "\n",
    "    <full-instructions header=\"Transcription Instructions\">\n",
    "        <h2>Instructions</h2>\n",
    "        <p>Click the play button once and listen carefully to the audio section clip. Type what you hear in the box\n",
    "            below.</p>\n",
    "    </full-instructions>\n",
    "\n",
    "</crowd-form>\n",
    "\"\"\"\n",
    "\n",
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = 'ui-transcribe-' + str(uuid.uuid4()) \n",
    "\n",
    "# Create task UI\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow definition name - this value is unique per account and region. You can also provide your own value here.\n",
    "flowDefinitionName = 'fd-transcribe-demo-' + str(uuid.uuid4()) \n",
    "\n",
    "create_workflow_definition_response = sagemaker.create_flow_definition(\n",
    "        FlowDefinitionName= flowDefinitionName,\n",
    "        RoleArn= ROLE,\n",
    "        HumanLoopConfig= {\n",
    "            \"WorkteamArn\": WORKTEAM_ARN,\n",
    "            \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "            \"TaskCount\": 1,\n",
    "            \"TaskDescription\": \"Identify the word(s) spoken in the provided audio clip\",\n",
    "            \"TaskTitle\": \"Determine Words/Phrases of Audio Clip\"\n",
    "        },\n",
    "        OutputConfig={\n",
    "            \"S3OutputPath\" : OUTPUT_PATH\n",
    "        }\n",
    "    )\n",
    "flowDefinitionArn = create_workflow_definition_response['FlowDefinitionArn'] # let's save this ARN for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe flow definition - status should be active\n",
    "for x in range(60):\n",
    "    describeFlowDefinitionResponse = sagemaker.describe_flow_definition(FlowDefinitionName=flowDefinitionName)\n",
    "    print(describeFlowDefinitionResponse['FlowDefinitionStatus'])\n",
    "    if (describeFlowDefinitionResponse['FlowDefinitionStatus'] == 'Active'):\n",
    "        print(\"Flow Definition is active\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Loops\n",
    "### Sending sequences of words/phrases of low confidence for review\n",
    "As we iterate through the list of words and their confidence scores, we create a HumanLoop task whenever the confidence score is below a threshold. The task consists of a sequence of words \"neighboring\" the word with low confidence, since it is possible that nearby words/phrases were also mis-transcribed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to get the words near a word with poor confidence,\n",
    "# since it is possible that the transcription also mis-transcribed nearby words/phrases\n",
    "def get_word_neighbors(words, index):\n",
    "    \"\"\"\n",
    "    gets the words transcribe found at most 3 away from the input index\n",
    "    Returns:\n",
    "        list: words at most 3 away from the input index\n",
    "        int: starting time of the first word in the list\n",
    "        int: ending time of the last word in the list\n",
    "    \"\"\"\n",
    "    i = max(0, index - 3)\n",
    "    j = min(len(words) - 1, index + 3)\n",
    "    return words[i: j + 1], words[i][\"start_time\"], words[j][\"end_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data, human loop started\n",
    "human_loops_started = []\n",
    "CONFIDENCE_SCORE_THRESHOLD = .4\n",
    "i = 0\n",
    "for obj in confidences_1:\n",
    "    word = obj[\"content\"]\n",
    "    neighbors, start_time, end_time = get_word_neighbors(confidences_1, i)\n",
    "    \n",
    "#     print(f'Processing word: \\\"{obj[\"content\"]}\\\"')\n",
    "    \n",
    "    # Our condition for when we want to engage a human for review\n",
    "    if (obj[\"confidence\"] < CONFIDENCE_SCORE_THRESHOLD):\n",
    "        \n",
    "        # get the original sequence of words\n",
    "        sequence = \"\"\n",
    "        for block in neighbors:\n",
    "            sequence += block['content'] + \" \"\n",
    "        \n",
    "        humanLoopName = str(uuid.uuid4())\n",
    "        # \"initialValue\": word,\n",
    "        inputContent = {\n",
    "            \"audioPath\": job_uri_s3,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"original_words\": sequence\n",
    "        }\n",
    "        start_loop_response = a2i.start_human_loop(\n",
    "            HumanLoopName=humanLoopName,\n",
    "            FlowDefinitionArn=flowDefinitionArn,\n",
    "            HumanLoopInput={\n",
    "                \"InputContent\": json.dumps(inputContent)\n",
    "            }\n",
    "        )\n",
    "        human_loops_started.append(humanLoopName)\n",
    "        print(f'Confidence score of {obj[\"confidence\"]} is less than the threshold of {CONFIDENCE_SCORE_THRESHOLD}')\n",
    "        print(f'Starting human loop with name: {humanLoopName}')\n",
    "        print(f'Sending words from times {start_time} to {end_time} to review')\n",
    "        print(f'The original transcription is \"\"{sequence}\"\" \\n')\n",
    "#     else:\n",
    "# #         print(f'SentimentScore of {obj[\"confidence\"]} is above threshold of {CONFIDENCE_SCORE_THRESHOLD}')\n",
    "# #         print('No human loop created. \\n')\n",
    "\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait For Workers to Complete Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait For Workers to Complete Task\n",
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\")\n",
    "print('https://' + sagemaker.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Task Results\n",
    "\n",
    "Once work is completed, Amazon A2I stores results in your S3 bucket and sends a Cloudwatch event. Your results should be available in the S3 OUTPUT_PATH when all work is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "    output_bucket_key = splitted_string[1]\n",
    "\n",
    "    response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    pp.pprint(json_output)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Technical Terms\n",
    "To get the technical terms identified by human review, we accumulate all human-reviewed words into a list and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_words = []\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "    output_bucket_key = splitted_string[1]\n",
    "\n",
    "    response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    \n",
    "    # add the human-reviewed answers split by spaces\n",
    "    corrected_words += json_output['humanAnswers'][0]['answerContent']['transcription'].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of English words\n",
    "# Note that this corpus of words is not 100% exhaustive\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "my_dict=set(words.words()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting words not in the English language corpus\n",
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text)\n",
    "    english_vocab = set(w.lower() for w in my_dict)\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return sorted(unusual)\n",
    "\n",
    "# Function for removing contractions\n",
    "# https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "def remove_contractions(word_list):\n",
    "    return [word for word in word_list if word not in contractions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Technical/Uncommon Words\n",
    "After removing contractions, human-reviewed words that are not in the English language corpus are likely to be the technical terms we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in remove_contractions(corrected_words):\n",
    "    if word not in my_dict:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Vocabulary\n",
    "Using the technical terms identified above, we've created a custom vocabulary table. A custom vocabulary table enables options to tell Amazon Transcribe how each technical term is pronounced and how it should be displayed.\n",
    "\n",
    "More details on how to form a custom vocabulary table can be found here: https://docs.aws.amazon.com/transcribe/latest/dg/how-vocabulary.html#create-vocabulary-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_words=[['Phrase','IPA','SoundsLike','DisplayAs'],\n",
    "                 ['E.C.-Two','','ee-see-too','EC2'],\n",
    "                 ['E.C.-Two-instance','','ee-see-too-in-stunce','EC2 instance'],\n",
    "                 ['lambda','','lam-duh','Lambda'],\n",
    "                 ['S.D.K.','','ess-dee-kay','SDK'],\n",
    "                 ['boto-three','','boe-toe-three','Boto3'],\n",
    "                 ['S.-Three','','ess-three','S3'],\n",
    "                 ['github','','git-hub','Github'],\n",
    "                 ['sagemaker','','sage-may-ker','SageMaker'],\n",
    "                 ['E.B.S.','','ee-bee-ess','EBS'],\n",
    "                 ['G.P.U.','','gee-pee-you','GPU'],\n",
    "                 ['git-repository','','git-ree-paw-zih-tor-ee','Git repository'],\n",
    "                 ['jupyter','','joo-pih-ter','Jupyter'],\n",
    "                 ['kernel','','ker-null','kernel'],\n",
    "                 ['config','','con-fig','config'],\n",
    "                 ['configs','','con-figs','configs'],\n",
    "                 ['D.B.-pedia','','dee-bee-pee-dee-yuh','dbpedia'],\n",
    "                 ['git-clone','','','git clone'],\n",
    "                 ['notebook-instance','','','notebook instance'],\n",
    "                 ['V.P.C.','','','VPC'],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Table to a Txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vocab_file_name = \"customvocab3.txt\"\n",
    "file1 = open(custom_vocab_file,\"w\")\n",
    "template = '{}\\t{}\\t{}\\t{}\\n'\n",
    "for line in finalized_words:\n",
    "    file1.write(template.format(line[0],\n",
    "                                line[1],\n",
    "                                line[2],\n",
    "                                line[3])\n",
    "               )\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Custom Vocabulary File to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file(custom_vocab_file_name, BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Custom Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe = boto3.client(\"transcribe\")\n",
    "response = transcribe.create_vocabulary(\n",
    "    VocabularyName='aws-sagemaker-vocab-4',\n",
    "    LanguageCode='en-US',\n",
    "    VocabularyFileUri='s3://' + BUCKET + '/' + custom_vocab_file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the status of the vocab you created again (must wait until its VocabularyState is READY)\n",
    "response2 = transcribe.get_vocabulary(\n",
    "    VocabularyName='aws-sagemaker-vocab-4'\n",
    ")\n",
    "pp.pprint(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-transcribe using the Custom Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_2='AWS-sage-improved-1'\n",
    "vocab_improved='aws-sagemaker-vocab-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe(job_name_2, job_uri_s3, BUCKET, vocab_name=vocab_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_transcript_2,sentences_and_times_2, confidences_2, scores_2 = get_transcript_text_and_timestamps(BUCKET,\n",
    "                                                                                                      job_name_2+\".json\")\n",
    "                                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Improved Transcript to Txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the improved transcript\n",
    "file4 = open(\"improvedtranscript_2.txt\",\"w\") \n",
    "for tup in sentences_and_times_2:\n",
    "    file4.write(tup['sentence'] + \"\\n\") \n",
    "file4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Word Error Rate (WER)\n",
    "We'll be using a lightweight open-source Python library called JiWER for calculating WER between transcripts.\n",
    "\n",
    "For more details, see: https://pypi.org/project/jiwer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small example\n",
    "ground_truth = \"hello world\"\n",
    "hypothesis = \"hello duck\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer(ground_truth, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformation function to preprocess transcript\n",
    "transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.SentencesToListOfWords(),\n",
    "    jiwer.SentencesToListOfWords(word_delimiter=\" \"),\n",
    "    jiwer.RemoveEmptyStrings()\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the original transcript (before applying the custom vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_2_original = \"\"\n",
    "f3 = open(\"originaltranscript.txt\", \"r\")\n",
    "for line in f3:\n",
    "    if line.strip() == \"--STOP--\":\n",
    "        break\n",
    "    hypothesis_2_original += (line.strip() + \" \")\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the \"Ground Truth\" transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_2 = \"\"\n",
    "f1 = open(\"ground_truth.txt\", \"r\")\n",
    "for line in f1:\n",
    "    if line.strip() == \"--STOP--\":\n",
    "        break\n",
    "    ground_truth_2 += (line.strip() + \" \")\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the new transcript (after applying the custom vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_2 = \"\"\n",
    "f2 = open(\"improvedtranscript_2.txt\", \"r\")\n",
    "for line in f2:\n",
    "    if line.strip() == \"--STOP--\":\n",
    "        break\n",
    "    hypothesis_2 += (line.strip() + \" \")\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jiwer.wer(\n",
    "    ground_truth_2, \n",
    "    hypothesis_2_original, \n",
    "    truth_transform=transformation, \n",
    "    hypothesis_transform=transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute New Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jiwer.wer(\n",
    "    ground_truth_2, \n",
    "    hypothesis_2, \n",
    "    truth_transform=transformation, \n",
    "    hypothesis_transform=transformation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
